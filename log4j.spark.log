17/03/28 10:23:33 INFO SparkContext: Running Spark version 2.0.1
17/03/28 10:23:33 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:379)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:394)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:387)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:80)
	at org.apache.hadoop.security.SecurityUtil.getAuthenticationMethod(SecurityUtil.java:611)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:273)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:261)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:791)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:761)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:634)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2265)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2265)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2265)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:294)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2275)
	at org.apache.spark.SparkContext.getOrCreate(SparkContext.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at sparklyr.Invoke$.invoke(invoke.scala:94)
	at sparklyr.StreamHandler$.handleMethodCall(stream.scala:89)
	at sparklyr.StreamHandler$.read(stream.scala:55)
	at sparklyr.BackendHandler.channelRead0(handler.scala:49)
	at sparklyr.BackendHandler.channelRead0(handler.scala:14)
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:244)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137)
	at java.lang.Thread.run(Thread.java:745)
17/03/28 10:23:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/03/28 10:23:33 INFO SecurityManager: Changing view acls to: lifeng
17/03/28 10:23:33 INFO SecurityManager: Changing modify acls to: lifeng
17/03/28 10:23:33 INFO SecurityManager: Changing view acls groups to: 
17/03/28 10:23:33 INFO SecurityManager: Changing modify acls groups to: 
17/03/28 10:23:33 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lifeng); groups with view permissions: Set(); users  with modify permissions: Set(lifeng); groups with modify permissions: Set()
17/03/28 10:23:33 INFO Utils: Successfully started service 'sparkDriver' on port 63788.
17/03/28 10:23:33 INFO SparkEnv: Registering MapOutputTracker
17/03/28 10:23:33 INFO SparkEnv: Registering BlockManagerMaster
17/03/28 10:23:34 INFO DiskBlockManager: Created local directory at C:\Users\lifeng\AppData\Local\Temp\blockmgr-497769e1-dfce-4655-bd3d-70f19723d912
17/03/28 10:23:34 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
17/03/28 10:23:34 INFO SparkEnv: Registering OutputCommitCoordinator
17/03/28 10:23:34 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/03/28 10:23:34 INFO SparkUI: Bound SparkUI to 127.0.0.1, and started at http://127.0.0.1:4040
17/03/28 10:23:34 INFO SparkContext: Added JAR file:/D:/R/R-3.3.3/library/sparklyr/java/sparklyr-2.0-2.11.jar at spark://127.0.0.1:63788/jars/sparklyr-2.0-2.11.jar with timestamp 1490667814483
17/03/28 10:23:34 INFO Executor: Starting executor ID driver on host localhost
17/03/28 10:23:34 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 63809.
17/03/28 10:23:34 INFO NettyBlockTransferService: Server created on 127.0.0.1:63809
17/03/28 10:23:34 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 127.0.0.1, 63809)
17/03/28 10:23:34 INFO BlockManagerMasterEndpoint: Registering block manager 127.0.0.1:63809 with 366.3 MB RAM, BlockManagerId(driver, 127.0.0.1, 63809)
17/03/28 10:23:34 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 127.0.0.1, 63809)
17/03/28 10:23:34 WARN SparkContext: Use an existing SparkContext, some configuration may not take effect.
17/03/28 10:23:34 WARN SparkContext: Use an existing SparkContext, some configuration may not take effect.
17/03/28 10:23:34 INFO HiveSharedState: Warehouse path is 'D:Sparkspark-2.0.1-bin-hadoop2.7	mphive'.
17/03/28 11:11:31 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/03/28 11:11:31 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
17/03/28 11:11:32 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/03/28 11:11:32 INFO ObjectStore: ObjectStore, initialize called
17/03/28 11:11:32 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
17/03/28 11:11:32 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
17/03/28 11:11:34 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
17/03/28 11:11:34 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 11:11:34 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 11:11:35 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 11:11:35 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 11:11:35 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
17/03/28 11:11:35 INFO ObjectStore: Initialized ObjectStore
17/03/28 11:11:35 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
17/03/28 11:11:35 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
17/03/28 11:11:35 INFO HiveMetaStore: Added admin role in metastore
17/03/28 11:11:35 INFO HiveMetaStore: Added public role in metastore
17/03/28 11:11:36 INFO HiveMetaStore: No user is added in admin role, since config is empty
17/03/28 11:11:36 INFO HiveMetaStore: 0: get_all_databases
17/03/28 11:11:36 INFO audit: ugi=lifeng	ip=unknown-ip-addr	cmd=get_all_databases	
17/03/28 11:11:36 INFO HiveMetaStore: 0: get_functions: db=default pat=*
17/03/28 11:11:36 INFO audit: ugi=lifeng	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
17/03/28 11:11:36 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 11:12:49 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/03/28 11:12:49 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
17/03/28 11:12:50 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/03/28 11:12:50 INFO ObjectStore: ObjectStore, initialize called
17/03/28 11:12:50 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
17/03/28 11:12:50 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
17/03/28 11:12:51 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
17/03/28 11:12:52 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 11:12:52 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 11:12:52 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 11:12:52 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 11:12:53 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
17/03/28 11:12:53 INFO ObjectStore: Initialized ObjectStore
17/03/28 11:12:53 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
17/03/28 11:12:53 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
17/03/28 11:12:53 INFO HiveMetaStore: Added admin role in metastore
17/03/28 11:12:53 INFO HiveMetaStore: Added public role in metastore
17/03/28 11:12:53 INFO HiveMetaStore: No user is added in admin role, since config is empty
17/03/28 11:12:53 INFO HiveMetaStore: 0: get_all_databases
17/03/28 11:12:53 INFO audit: ugi=lifeng	ip=unknown-ip-addr	cmd=get_all_databases	
17/03/28 11:12:53 INFO HiveMetaStore: 0: get_functions: db=default pat=*
17/03/28 11:12:53 INFO audit: ugi=lifeng	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
17/03/28 11:12:53 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 11:16:16 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/03/28 11:16:16 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
17/03/28 11:16:17 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/03/28 11:16:17 INFO ObjectStore: ObjectStore, initialize called
17/03/28 11:16:17 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
17/03/28 11:16:17 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
17/03/28 11:16:18 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
17/03/28 11:16:19 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 11:16:19 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 11:16:19 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 11:16:19 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 11:16:19 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
17/03/28 11:16:19 INFO ObjectStore: Initialized ObjectStore
17/03/28 11:16:19 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
17/03/28 11:16:20 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
17/03/28 11:16:20 INFO HiveMetaStore: Added admin role in metastore
17/03/28 11:16:20 INFO HiveMetaStore: Added public role in metastore
17/03/28 11:16:20 INFO HiveMetaStore: No user is added in admin role, since config is empty
17/03/28 11:16:20 INFO HiveMetaStore: 0: get_all_databases
17/03/28 11:16:20 INFO audit: ugi=lifeng	ip=unknown-ip-addr	cmd=get_all_databases	
17/03/28 11:16:20 INFO HiveMetaStore: 0: get_functions: db=default pat=*
17/03/28 11:16:20 INFO audit: ugi=lifeng	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
17/03/28 11:16:20 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 11:16:49 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/03/28 11:16:49 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
17/03/28 11:16:49 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/03/28 11:16:49 INFO ObjectStore: ObjectStore, initialize called
17/03/28 11:16:50 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
17/03/28 11:16:50 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
17/03/28 11:16:50 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
17/03/28 11:16:51 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 11:16:51 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 11:16:52 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 11:16:52 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 11:16:52 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
17/03/28 11:16:52 INFO ObjectStore: Initialized ObjectStore
17/03/28 11:16:52 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
17/03/28 11:16:52 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
17/03/28 11:16:52 INFO HiveMetaStore: Added admin role in metastore
17/03/28 11:16:52 INFO HiveMetaStore: Added public role in metastore
17/03/28 11:16:52 INFO HiveMetaStore: No user is added in admin role, since config is empty
17/03/28 11:16:52 INFO HiveMetaStore: 0: get_all_databases
17/03/28 11:16:52 INFO audit: ugi=lifeng	ip=unknown-ip-addr	cmd=get_all_databases	
17/03/28 11:16:52 INFO HiveMetaStore: 0: get_functions: db=default pat=*
17/03/28 11:16:52 INFO audit: ugi=lifeng	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
17/03/28 11:16:52 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 11:17:28 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/03/28 11:17:28 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
17/03/28 11:17:29 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/03/28 11:17:29 INFO ObjectStore: ObjectStore, initialize called
17/03/28 11:17:29 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
17/03/28 11:17:29 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
17/03/28 11:17:30 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
17/03/28 11:17:30 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 11:17:30 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 11:17:31 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 11:17:31 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 11:17:31 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
17/03/28 11:17:31 INFO ObjectStore: Initialized ObjectStore
17/03/28 11:17:31 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
17/03/28 11:17:31 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
17/03/28 11:17:31 INFO HiveMetaStore: Added admin role in metastore
17/03/28 11:17:31 INFO HiveMetaStore: Added public role in metastore
17/03/28 11:17:31 INFO HiveMetaStore: No user is added in admin role, since config is empty
17/03/28 11:17:31 INFO HiveMetaStore: 0: get_all_databases
17/03/28 11:17:31 INFO audit: ugi=lifeng	ip=unknown-ip-addr	cmd=get_all_databases	
17/03/28 11:17:31 INFO HiveMetaStore: 0: get_functions: db=default pat=*
17/03/28 11:17:31 INFO audit: ugi=lifeng	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
17/03/28 11:17:31 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 11:17:51 INFO SparkContext: Invoking stop() from shutdown hook
17/03/28 11:17:51 INFO SparkUI: Stopped Spark web UI at http://127.0.0.1:4040
17/03/28 11:17:51 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
17/03/28 11:17:51 INFO MemoryStore: MemoryStore cleared
17/03/28 11:17:51 INFO BlockManager: BlockManager stopped
17/03/28 11:17:51 INFO BlockManagerMaster: BlockManagerMaster stopped
17/03/28 11:17:51 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
17/03/28 11:17:51 INFO SparkContext: Successfully stopped SparkContext
17/03/28 11:17:51 INFO ShutdownHookManager: Shutdown hook called
17/03/28 11:17:51 INFO ShutdownHookManager: Deleting directory C:\Users\lifeng\AppData\Local\Temp\spark-3f6bd008-1cd3-4038-b9aa-fbcf7814f5c3
17/03/28 11:18:50 INFO SparkContext: Running Spark version 2.0.1
17/03/28 11:18:51 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:379)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:394)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:387)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:80)
	at org.apache.hadoop.security.SecurityUtil.getAuthenticationMethod(SecurityUtil.java:611)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:273)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:261)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:791)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:761)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:634)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2265)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2265)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2265)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:294)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2275)
	at org.apache.spark.SparkContext.getOrCreate(SparkContext.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at sparklyr.Invoke$.invoke(invoke.scala:94)
	at sparklyr.StreamHandler$.handleMethodCall(stream.scala:89)
	at sparklyr.StreamHandler$.read(stream.scala:55)
	at sparklyr.BackendHandler.channelRead0(handler.scala:49)
	at sparklyr.BackendHandler.channelRead0(handler.scala:14)
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:244)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137)
	at java.lang.Thread.run(Thread.java:745)
17/03/28 11:18:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/03/28 11:18:51 INFO SecurityManager: Changing view acls to: lifeng
17/03/28 11:18:51 INFO SecurityManager: Changing modify acls to: lifeng
17/03/28 11:18:51 INFO SecurityManager: Changing view acls groups to: 
17/03/28 11:18:51 INFO SecurityManager: Changing modify acls groups to: 
17/03/28 11:18:51 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lifeng); groups with view permissions: Set(); users  with modify permissions: Set(lifeng); groups with modify permissions: Set()
17/03/28 11:18:51 INFO Utils: Successfully started service 'sparkDriver' on port 64154.
17/03/28 11:18:51 INFO SparkEnv: Registering MapOutputTracker
17/03/28 11:18:51 INFO SparkEnv: Registering BlockManagerMaster
17/03/28 11:18:51 INFO DiskBlockManager: Created local directory at C:\Users\lifeng\AppData\Local\Temp\blockmgr-682b5a2b-3b7c-46b0-8f0e-4b90039a13e0
17/03/28 11:18:51 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
17/03/28 11:18:51 INFO SparkEnv: Registering OutputCommitCoordinator
17/03/28 11:18:51 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/03/28 11:18:51 INFO SparkUI: Bound SparkUI to 127.0.0.1, and started at http://127.0.0.1:4040
17/03/28 11:18:51 INFO SparkContext: Added JAR file:/D:/R/R-3.3.3/library/sparklyr/java/sparklyr-2.0-2.11.jar at spark://127.0.0.1:64154/jars/sparklyr-2.0-2.11.jar with timestamp 1490671131924
17/03/28 11:18:51 INFO Executor: Starting executor ID driver on host localhost
17/03/28 11:18:52 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 64176.
17/03/28 11:18:52 INFO NettyBlockTransferService: Server created on 127.0.0.1:64176
17/03/28 11:18:52 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 127.0.0.1, 64176)
17/03/28 11:18:52 INFO BlockManagerMasterEndpoint: Registering block manager 127.0.0.1:64176 with 366.3 MB RAM, BlockManagerId(driver, 127.0.0.1, 64176)
17/03/28 11:18:52 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 127.0.0.1, 64176)
17/03/28 11:18:52 WARN SparkContext: Use an existing SparkContext, some configuration may not take effect.
17/03/28 11:18:52 WARN SparkContext: Use an existing SparkContext, some configuration may not take effect.
17/03/28 11:18:52 INFO HiveSharedState: Warehouse path is 'D:Sparkspark-2.0.1-bin-hadoop2.7	mphive'.
17/03/28 11:19:01 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/03/28 11:19:01 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
17/03/28 11:19:02 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/03/28 11:19:02 INFO ObjectStore: ObjectStore, initialize called
17/03/28 11:19:02 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
17/03/28 11:19:02 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
17/03/28 11:19:03 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
17/03/28 11:19:04 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 11:19:04 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 11:19:05 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 11:19:05 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 11:19:05 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
17/03/28 11:19:05 INFO ObjectStore: Initialized ObjectStore
17/03/28 11:19:05 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
17/03/28 11:19:05 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
17/03/28 11:19:05 INFO HiveMetaStore: Added admin role in metastore
17/03/28 11:19:05 INFO HiveMetaStore: Added public role in metastore
17/03/28 11:19:05 INFO HiveMetaStore: No user is added in admin role, since config is empty
17/03/28 11:19:06 INFO HiveMetaStore: 0: get_all_databases
17/03/28 11:19:06 INFO audit: ugi=lifeng	ip=unknown-ip-addr	cmd=get_all_databases	
17/03/28 11:19:06 INFO HiveMetaStore: 0: get_functions: db=default pat=*
17/03/28 11:19:06 INFO audit: ugi=lifeng	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
17/03/28 11:19:06 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 11:41:37 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/03/28 11:41:37 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
17/03/28 11:41:38 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/03/28 11:41:38 INFO ObjectStore: ObjectStore, initialize called
17/03/28 11:41:38 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
17/03/28 11:41:38 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
17/03/28 11:41:39 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
17/03/28 11:41:40 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 11:41:40 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 11:41:40 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 11:41:40 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 11:41:40 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
17/03/28 11:41:40 INFO ObjectStore: Initialized ObjectStore
17/03/28 11:41:40 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
17/03/28 11:41:41 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
17/03/28 11:41:41 INFO HiveMetaStore: Added admin role in metastore
17/03/28 11:41:41 INFO HiveMetaStore: Added public role in metastore
17/03/28 11:41:41 INFO HiveMetaStore: No user is added in admin role, since config is empty
17/03/28 11:41:41 INFO HiveMetaStore: 0: get_all_databases
17/03/28 11:41:41 INFO audit: ugi=lifeng	ip=unknown-ip-addr	cmd=get_all_databases	
17/03/28 11:41:41 INFO HiveMetaStore: 0: get_functions: db=default pat=*
17/03/28 11:41:41 INFO audit: ugi=lifeng	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
17/03/28 11:41:41 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 11:42:51 INFO SparkContext: Invoking stop() from shutdown hook
17/03/28 11:42:51 INFO SparkUI: Stopped Spark web UI at http://127.0.0.1:4040
17/03/28 11:42:51 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
17/03/28 11:42:51 INFO MemoryStore: MemoryStore cleared
17/03/28 11:42:51 INFO BlockManager: BlockManager stopped
17/03/28 11:42:51 INFO BlockManagerMaster: BlockManagerMaster stopped
17/03/28 11:42:51 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
17/03/28 11:42:51 INFO SparkContext: Successfully stopped SparkContext
17/03/28 11:42:51 INFO ShutdownHookManager: Shutdown hook called
17/03/28 11:42:51 INFO ShutdownHookManager: Deleting directory C:\Users\lifeng\AppData\Local\Temp\spark-c1caa83a-5fb4-4ac1-abae-f89d149b9ddc
17/03/28 11:43:13 INFO SparkContext: Running Spark version 2.0.1
17/03/28 11:43:13 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:379)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:394)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:387)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:80)
	at org.apache.hadoop.security.SecurityUtil.getAuthenticationMethod(SecurityUtil.java:611)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:273)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:261)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:791)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:761)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:634)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2265)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2265)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2265)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:294)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2275)
	at org.apache.spark.SparkContext.getOrCreate(SparkContext.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at sparklyr.Invoke$.invoke(invoke.scala:94)
	at sparklyr.StreamHandler$.handleMethodCall(stream.scala:89)
	at sparklyr.StreamHandler$.read(stream.scala:55)
	at sparklyr.BackendHandler.channelRead0(handler.scala:49)
	at sparklyr.BackendHandler.channelRead0(handler.scala:14)
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:244)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137)
	at java.lang.Thread.run(Thread.java:745)
17/03/28 11:43:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/03/28 11:43:13 INFO SecurityManager: Changing view acls to: lifeng
17/03/28 11:43:13 INFO SecurityManager: Changing modify acls to: lifeng
17/03/28 11:43:13 INFO SecurityManager: Changing view acls groups to: 
17/03/28 11:43:13 INFO SecurityManager: Changing modify acls groups to: 
17/03/28 11:43:13 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lifeng); groups with view permissions: Set(); users  with modify permissions: Set(lifeng); groups with modify permissions: Set()
17/03/28 11:43:14 INFO Utils: Successfully started service 'sparkDriver' on port 50398.
17/03/28 11:43:14 INFO SparkEnv: Registering MapOutputTracker
17/03/28 11:43:14 INFO SparkEnv: Registering BlockManagerMaster
17/03/28 11:43:14 INFO DiskBlockManager: Created local directory at C:\Users\lifeng\AppData\Local\Temp\blockmgr-47690b66-b2f2-4fc5-9dd1-facd1c8873f9
17/03/28 11:43:14 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
17/03/28 11:43:14 INFO SparkEnv: Registering OutputCommitCoordinator
17/03/28 11:43:14 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/03/28 11:43:14 INFO SparkUI: Bound SparkUI to 127.0.0.1, and started at http://127.0.0.1:4040
17/03/28 11:43:14 INFO SparkContext: Added JAR file:/D:/R/R-3.3.3/library/sparklyr/java/sparklyr-2.0-2.11.jar at spark://127.0.0.1:50398/jars/sparklyr-2.0-2.11.jar with timestamp 1490672594654
17/03/28 11:43:14 INFO Executor: Starting executor ID driver on host localhost
17/03/28 11:43:14 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50419.
17/03/28 11:43:14 INFO NettyBlockTransferService: Server created on 127.0.0.1:50419
17/03/28 11:43:14 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 127.0.0.1, 50419)
17/03/28 11:43:14 INFO BlockManagerMasterEndpoint: Registering block manager 127.0.0.1:50419 with 366.3 MB RAM, BlockManagerId(driver, 127.0.0.1, 50419)
17/03/28 11:43:14 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 127.0.0.1, 50419)
17/03/28 11:43:14 WARN SparkContext: Use an existing SparkContext, some configuration may not take effect.
17/03/28 11:43:15 WARN SparkContext: Use an existing SparkContext, some configuration may not take effect.
17/03/28 11:43:15 INFO HiveSharedState: Warehouse path is 'D:Sparkspark-2.0.1-bin-hadoop2.7	mphive'.
17/03/28 11:43:15 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/03/28 11:43:15 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
17/03/28 11:43:16 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/03/28 11:43:16 INFO ObjectStore: ObjectStore, initialize called
17/03/28 11:43:16 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
17/03/28 11:43:16 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
17/03/28 11:43:17 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
17/03/28 11:43:18 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 11:43:18 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 11:43:18 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 11:43:18 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 11:43:18 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
17/03/28 11:43:18 INFO ObjectStore: Initialized ObjectStore
17/03/28 11:43:19 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
17/03/28 11:43:19 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
17/03/28 11:43:19 INFO HiveMetaStore: Added admin role in metastore
17/03/28 11:43:19 INFO HiveMetaStore: Added public role in metastore
17/03/28 11:43:19 INFO HiveMetaStore: No user is added in admin role, since config is empty
17/03/28 11:43:19 INFO HiveMetaStore: 0: get_all_databases
17/03/28 11:43:19 INFO audit: ugi=lifeng	ip=unknown-ip-addr	cmd=get_all_databases	
17/03/28 11:43:19 INFO HiveMetaStore: 0: get_functions: db=default pat=*
17/03/28 11:43:19 INFO audit: ugi=lifeng	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
17/03/28 11:43:19 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 11:44:09 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/03/28 11:44:09 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
17/03/28 11:44:09 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/03/28 11:44:09 INFO ObjectStore: ObjectStore, initialize called
17/03/28 11:44:09 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
17/03/28 11:44:09 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
17/03/28 11:44:11 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
17/03/28 11:44:11 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 11:44:11 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 11:44:12 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 11:44:12 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 11:44:12 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
17/03/28 11:44:12 INFO ObjectStore: Initialized ObjectStore
17/03/28 11:44:12 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
17/03/28 11:44:12 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
17/03/28 11:44:12 INFO HiveMetaStore: Added admin role in metastore
17/03/28 11:44:12 INFO HiveMetaStore: Added public role in metastore
17/03/28 11:44:12 INFO HiveMetaStore: No user is added in admin role, since config is empty
17/03/28 11:44:12 INFO HiveMetaStore: 0: get_all_databases
17/03/28 11:44:12 INFO audit: ugi=lifeng	ip=unknown-ip-addr	cmd=get_all_databases	
17/03/28 11:44:12 INFO HiveMetaStore: 0: get_functions: db=default pat=*
17/03/28 11:44:12 INFO audit: ugi=lifeng	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
17/03/28 11:44:12 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 11:44:28 INFO SparkContext: Invoking stop() from shutdown hook
17/03/28 11:44:28 INFO SparkUI: Stopped Spark web UI at http://127.0.0.1:4040
17/03/28 11:44:28 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
17/03/28 11:44:28 INFO MemoryStore: MemoryStore cleared
17/03/28 11:44:28 INFO BlockManager: BlockManager stopped
17/03/28 11:44:28 INFO BlockManagerMaster: BlockManagerMaster stopped
17/03/28 11:44:28 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
17/03/28 11:44:28 INFO SparkContext: Successfully stopped SparkContext
17/03/28 11:44:28 INFO ShutdownHookManager: Shutdown hook called
17/03/28 11:44:28 INFO ShutdownHookManager: Deleting directory C:\Users\lifeng\AppData\Local\Temp\spark-bd3e6ed5-cdc6-4ad6-a92e-e39e824522f9
17/03/28 11:45:11 INFO SparkContext: Running Spark version 1.6.2
17/03/28 11:45:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/03/28 11:45:11 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:355)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:370)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:363)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:104)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:86)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:66)
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:280)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:271)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:248)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:763)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:748)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:621)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2198)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2198)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2198)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:322)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2281)
	at org.apache.spark.SparkContext.getOrCreate(SparkContext.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at sparklyr.Invoke$.invoke(invoke.scala:94)
	at sparklyr.StreamHandler$.handleMethodCall(stream.scala:89)
	at sparklyr.StreamHandler$.read(stream.scala:55)
	at sparklyr.BackendHandler.channelRead0(handler.scala:49)
	at sparklyr.BackendHandler.channelRead0(handler.scala:14)
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:244)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137)
	at java.lang.Thread.run(Thread.java:745)
17/03/28 11:45:11 INFO SecurityManager: Changing view acls to: lifeng
17/03/28 11:45:11 INFO SecurityManager: Changing modify acls to: lifeng
17/03/28 11:45:11 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(lifeng); users with modify permissions: Set(lifeng)
17/03/28 11:45:12 INFO Utils: Successfully started service 'sparkDriver' on port 50449.
17/03/28 11:45:12 INFO Slf4jLogger: Slf4jLogger started
17/03/28 11:45:12 INFO Remoting: Starting remoting
17/03/28 11:45:12 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@127.0.0.1:50462]
17/03/28 11:45:12 INFO Utils: Successfully started service 'sparkDriverActorSystem' on port 50462.
17/03/28 11:45:12 INFO SparkEnv: Registering MapOutputTracker
17/03/28 11:45:12 INFO SparkEnv: Registering BlockManagerMaster
17/03/28 11:45:12 INFO DiskBlockManager: Created local directory at C:\Users\lifeng\AppData\Local\Temp\blockmgr-2860ac1a-e701-48f2-91d4-b292ae94a950
17/03/28 11:45:12 INFO MemoryStore: MemoryStore started with capacity 511.1 MB
17/03/28 11:45:12 INFO SparkEnv: Registering OutputCommitCoordinator
17/03/28 11:45:13 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/03/28 11:45:13 INFO SparkUI: Started SparkUI at http://127.0.0.1:4040
17/03/28 11:45:13 INFO HttpFileServer: HTTP File server directory is C:\Users\lifeng\AppData\Local\Temp\spark-2d2610b5-cf1d-4816-b76c-be99305028b5\httpd-e4f02e9f-dacf-4c59-baca-7117e3d817d2
17/03/28 11:45:13 INFO HttpServer: Starting HTTP Server
17/03/28 11:45:13 INFO Utils: Successfully started service 'HTTP file server' on port 50465.
17/03/28 11:45:13 INFO SparkContext: Added JAR file:/D:/R/R-3.3.3/library/sparklyr/java/spark-csv_2.11-1.3.0.jar at http://127.0.0.1:50465/jars/spark-csv_2.11-1.3.0.jar with timestamp 1490672713090
17/03/28 11:45:13 INFO SparkContext: Added JAR file:/D:/R/R-3.3.3/library/sparklyr/java/commons-csv-1.1.jar at http://127.0.0.1:50465/jars/commons-csv-1.1.jar with timestamp 1490672713092
17/03/28 11:45:13 INFO SparkContext: Added JAR file:/D:/R/R-3.3.3/library/sparklyr/java/univocity-parsers-1.5.1.jar at http://127.0.0.1:50465/jars/univocity-parsers-1.5.1.jar with timestamp 1490672713094
17/03/28 11:45:13 INFO SparkContext: Added JAR file:/D:/R/R-3.3.3/library/sparklyr/java/sparklyr-1.6-2.10.jar at http://127.0.0.1:50465/jars/sparklyr-1.6-2.10.jar with timestamp 1490672713095
17/03/28 11:45:13 INFO Executor: Starting executor ID driver on host localhost
17/03/28 11:45:13 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50482.
17/03/28 11:45:13 INFO NettyBlockTransferService: Server created on 50482
17/03/28 11:45:13 INFO BlockManagerMaster: Trying to register BlockManager
17/03/28 11:45:13 INFO BlockManagerMasterEndpoint: Registering block manager localhost:50482 with 511.1 MB RAM, BlockManagerId(driver, localhost, 50482)
17/03/28 11:45:13 INFO BlockManagerMaster: Registered BlockManager
17/03/28 11:45:13 INFO HiveContext: Initializing execution hive, version 1.2.1
17/03/28 11:45:14 INFO ClientWrapper: Inspected Hadoop version: 2.6.0
17/03/28 11:45:14 INFO ClientWrapper: Loaded org.apache.hadoop.hive.shims.Hadoop23Shims for Hadoop version 2.6.0
17/03/28 11:45:14 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/03/28 11:45:14 INFO ObjectStore: ObjectStore, initialize called
17/03/28 11:45:14 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
17/03/28 11:45:14 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
17/03/28 11:45:14 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
17/03/28 11:45:14 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
17/03/28 11:45:21 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
17/03/28 11:45:22 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 11:45:22 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 11:45:27 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 11:45:27 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 11:45:28 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
17/03/28 11:45:28 INFO ObjectStore: Initialized ObjectStore
17/03/28 11:45:29 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
17/03/28 11:45:29 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
17/03/28 11:45:38 INFO HiveMetaStore: Added admin role in metastore
17/03/28 11:45:38 INFO HiveMetaStore: Added public role in metastore
17/03/28 11:45:39 INFO HiveMetaStore: No user is added in admin role, since config is empty
17/03/28 11:45:39 INFO HiveMetaStore: 0: get_all_databases
17/03/28 11:45:39 INFO audit: ugi=lifeng	ip=unknown-ip-addr	cmd=get_all_databases	
17/03/28 11:45:39 INFO HiveMetaStore: 0: get_functions: db=default pat=*
17/03/28 11:45:39 INFO audit: ugi=lifeng	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
17/03/28 11:45:39 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 11:45:41 INFO SparkContext: Starting job: collect at utils.scala:59
17/03/28 11:45:41 INFO DAGScheduler: Got job 0 (collect at utils.scala:59) with 1 output partitions
17/03/28 11:45:41 INFO DAGScheduler: Final stage: ResultStage 0 (collect at utils.scala:59)
17/03/28 11:45:41 INFO DAGScheduler: Parents of final stage: List()
17/03/28 11:45:41 INFO DAGScheduler: Missing parents: List()
17/03/28 11:45:41 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at map at utils.scala:56), which has no missing parents
17/03/28 11:45:41 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 5.4 KB, free 5.4 KB)
17/03/28 11:45:41 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.0 KB, free 8.4 KB)
17/03/28 11:45:41 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:50482 (size: 3.0 KB, free: 511.1 MB)
17/03/28 11:45:41 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1006
17/03/28 11:45:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at map at utils.scala:56)
17/03/28 11:45:41 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
17/03/28 11:45:41 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2348 bytes)
17/03/28 11:45:41 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
17/03/28 11:45:41 INFO Executor: Fetching http://127.0.0.1:50465/jars/univocity-parsers-1.5.1.jar with timestamp 1490672713094
17/03/28 11:45:41 INFO Utils: Fetching http://127.0.0.1:50465/jars/univocity-parsers-1.5.1.jar to C:\Users\lifeng\AppData\Local\Temp\spark-2d2610b5-cf1d-4816-b76c-be99305028b5\userFiles-db2f75f9-03b9-4273-964d-50a00a9700a9\fetchFileTemp5200045443623537489.tmp
17/03/28 11:45:41 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.NullPointerException
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1012)
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:482)
	at org.apache.hadoop.util.Shell.run(Shell.java:455)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:715)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:873)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:853)
	at org.apache.spark.util.Utils$.fetchFile(Utils.scala:407)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:430)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:422)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:422)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:206)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
17/03/28 11:45:41 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, localhost): java.lang.NullPointerException
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1012)
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:482)
	at org.apache.hadoop.util.Shell.run(Shell.java:455)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:715)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:873)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:853)
	at org.apache.spark.util.Utils$.fetchFile(Utils.scala:407)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:430)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:422)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:422)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:206)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

17/03/28 11:45:41 ERROR TaskSetManager: Task 0 in stage 0.0 failed 1 times; aborting job
17/03/28 11:45:41 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
17/03/28 11:45:41 INFO TaskSchedulerImpl: Cancelling stage 0
17/03/28 11:45:41 INFO DAGScheduler: ResultStage 0 (collect at utils.scala:59) failed in 0.229 s
17/03/28 11:45:41 INFO DAGScheduler: Job 0 failed: collect at utils.scala:59, took 0.551973 s
17/03/28 12:08:05 INFO BlockManagerInfo: Removed broadcast_0_piece0 on localhost:50482 in memory (size: 3.0 KB, free: 511.1 MB)
17/03/28 12:08:05 INFO ContextCleaner: Cleaned accumulator 3
17/03/28 12:08:05 INFO ContextCleaner: Cleaned accumulator 2
17/03/28 12:08:23 INFO SparkContext: Invoking stop() from shutdown hook
17/03/28 12:08:23 INFO SparkUI: Stopped Spark web UI at http://127.0.0.1:4040
17/03/28 12:08:23 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
17/03/28 12:08:23 INFO MemoryStore: MemoryStore cleared
17/03/28 12:08:23 INFO BlockManager: BlockManager stopped
17/03/28 12:08:23 INFO BlockManagerMaster: BlockManagerMaster stopped
17/03/28 12:08:23 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
17/03/28 12:08:23 INFO SparkContext: Successfully stopped SparkContext
17/03/28 12:08:23 INFO ShutdownHookManager: Shutdown hook called
17/03/28 12:08:23 INFO ShutdownHookManager: Deleting directory C:\Users\lifeng\AppData\Local\Temp\spark-da2f1688-e15d-4b3e-9a51-8912d28f20dd
17/03/28 12:08:23 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
17/03/28 12:08:23 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
17/03/28 12:08:23 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.
17/03/28 12:08:23 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\lifeng\AppData\Local\Temp\spark-da2f1688-e15d-4b3e-9a51-8912d28f20dd
java.io.IOException: Failed to delete: C:\Users\lifeng\AppData\Local\Temp\spark-da2f1688-e15d-4b3e-9a51-8912d28f20dd
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:929)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:267)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1801)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:218)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
17/03/28 12:08:23 INFO ShutdownHookManager: Deleting directory C:\Users\lifeng\AppData\Local\Temp\spark-2d2610b5-cf1d-4816-b76c-be99305028b5\httpd-e4f02e9f-dacf-4c59-baca-7117e3d817d2
17/03/28 12:08:23 INFO ShutdownHookManager: Deleting directory C:\Users\lifeng\AppData\Local\Temp\spark-2d2610b5-cf1d-4816-b76c-be99305028b5
17/03/28 12:08:43 INFO SparkContext: Running Spark version 2.0.1
17/03/28 12:08:43 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:379)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:394)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:387)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:80)
	at org.apache.hadoop.security.SecurityUtil.getAuthenticationMethod(SecurityUtil.java:611)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:273)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:261)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:791)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:761)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:634)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2265)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2265)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2265)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:294)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2275)
	at org.apache.spark.SparkContext.getOrCreate(SparkContext.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at sparklyr.Invoke$.invoke(invoke.scala:94)
	at sparklyr.StreamHandler$.handleMethodCall(stream.scala:89)
	at sparklyr.StreamHandler$.read(stream.scala:55)
	at sparklyr.BackendHandler.channelRead0(handler.scala:49)
	at sparklyr.BackendHandler.channelRead0(handler.scala:14)
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:244)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137)
	at java.lang.Thread.run(Thread.java:745)
17/03/28 12:08:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/03/28 12:08:43 INFO SecurityManager: Changing view acls to: lifeng
17/03/28 12:08:43 INFO SecurityManager: Changing modify acls to: lifeng
17/03/28 12:08:43 INFO SecurityManager: Changing view acls groups to: 
17/03/28 12:08:43 INFO SecurityManager: Changing modify acls groups to: 
17/03/28 12:08:43 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lifeng); groups with view permissions: Set(); users  with modify permissions: Set(lifeng); groups with modify permissions: Set()
17/03/28 12:08:43 INFO Utils: Successfully started service 'sparkDriver' on port 50785.
17/03/28 12:08:43 INFO SparkEnv: Registering MapOutputTracker
17/03/28 12:08:43 INFO SparkEnv: Registering BlockManagerMaster
17/03/28 12:08:43 INFO DiskBlockManager: Created local directory at C:\Users\lifeng\AppData\Local\Temp\blockmgr-e6e70412-7431-4760-8f0e-a467d15ed08b
17/03/28 12:08:43 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
17/03/28 12:08:44 INFO SparkEnv: Registering OutputCommitCoordinator
17/03/28 12:08:44 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/03/28 12:08:44 INFO SparkUI: Bound SparkUI to 127.0.0.1, and started at http://127.0.0.1:4040
17/03/28 12:08:44 INFO SparkContext: Added JAR file:/D:/R/R-3.3.3/library/sparklyr/java/sparklyr-2.0-2.11.jar at spark://127.0.0.1:50785/jars/sparklyr-2.0-2.11.jar with timestamp 1490674124229
17/03/28 12:08:44 INFO Executor: Starting executor ID driver on host localhost
17/03/28 12:08:44 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50806.
17/03/28 12:08:44 INFO NettyBlockTransferService: Server created on 127.0.0.1:50806
17/03/28 12:08:44 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 127.0.0.1, 50806)
17/03/28 12:08:44 INFO BlockManagerMasterEndpoint: Registering block manager 127.0.0.1:50806 with 366.3 MB RAM, BlockManagerId(driver, 127.0.0.1, 50806)
17/03/28 12:08:44 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 127.0.0.1, 50806)
17/03/28 12:08:44 WARN SparkContext: Use an existing SparkContext, some configuration may not take effect.
17/03/28 12:08:44 WARN SparkContext: Use an existing SparkContext, some configuration may not take effect.
17/03/28 12:08:44 INFO HiveSharedState: Warehouse path is 'D:Sparkspark-2.0.1-bin-hadoop2.7	mphive'.
17/03/28 12:08:44 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/03/28 12:08:45 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
17/03/28 12:08:45 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/03/28 12:08:45 INFO ObjectStore: ObjectStore, initialize called
17/03/28 12:08:45 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
17/03/28 12:08:45 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
17/03/28 12:08:46 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
17/03/28 12:08:47 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 12:08:47 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 12:08:48 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 12:08:48 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 12:08:48 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
17/03/28 12:08:48 INFO ObjectStore: Initialized ObjectStore
17/03/28 12:08:48 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
17/03/28 12:08:48 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
17/03/28 12:08:49 INFO HiveMetaStore: Added admin role in metastore
17/03/28 12:08:49 INFO HiveMetaStore: Added public role in metastore
17/03/28 12:08:49 INFO HiveMetaStore: No user is added in admin role, since config is empty
17/03/28 12:08:49 INFO HiveMetaStore: 0: get_all_databases
17/03/28 12:08:49 INFO audit: ugi=lifeng	ip=unknown-ip-addr	cmd=get_all_databases	
17/03/28 12:08:49 INFO HiveMetaStore: 0: get_functions: db=default pat=*
17/03/28 12:08:49 INFO audit: ugi=lifeng	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
17/03/28 12:08:49 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 12:10:47 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/03/28 12:10:47 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
17/03/28 12:10:48 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/03/28 12:10:48 INFO ObjectStore: ObjectStore, initialize called
17/03/28 12:10:48 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
17/03/28 12:10:48 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
17/03/28 12:10:49 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
17/03/28 12:10:50 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 12:10:50 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 12:10:50 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 12:10:50 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 12:10:50 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
17/03/28 12:10:50 INFO ObjectStore: Initialized ObjectStore
17/03/28 12:10:50 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
17/03/28 12:10:50 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
17/03/28 12:10:50 INFO HiveMetaStore: Added admin role in metastore
17/03/28 12:10:50 INFO HiveMetaStore: Added public role in metastore
17/03/28 12:10:50 INFO HiveMetaStore: No user is added in admin role, since config is empty
17/03/28 12:10:51 INFO HiveMetaStore: 0: get_all_databases
17/03/28 12:10:51 INFO audit: ugi=lifeng	ip=unknown-ip-addr	cmd=get_all_databases	
17/03/28 12:10:51 INFO HiveMetaStore: 0: get_functions: db=default pat=*
17/03/28 12:10:51 INFO audit: ugi=lifeng	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
17/03/28 12:10:51 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 12:11:53 INFO SparkContext: Invoking stop() from shutdown hook
17/03/28 12:11:53 INFO SparkUI: Stopped Spark web UI at http://127.0.0.1:4040
17/03/28 12:11:53 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
17/03/28 12:11:53 INFO MemoryStore: MemoryStore cleared
17/03/28 12:11:53 INFO BlockManager: BlockManager stopped
17/03/28 12:11:53 INFO BlockManagerMaster: BlockManagerMaster stopped
17/03/28 12:11:53 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
17/03/28 12:11:53 INFO SparkContext: Successfully stopped SparkContext
17/03/28 12:11:53 INFO ShutdownHookManager: Shutdown hook called
17/03/28 12:11:53 INFO ShutdownHookManager: Deleting directory C:\Users\lifeng\AppData\Local\Temp\spark-4e391a90-46fe-4fe4-9e44-14c813c045a2
17/03/28 12:21:55 INFO SparkContext: Running Spark version 1.6.2
17/03/28 12:21:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/03/28 12:21:55 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:355)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:370)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:363)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:104)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:86)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:66)
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:280)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:271)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:248)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:763)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:748)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:621)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2198)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2198)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2198)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:322)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2281)
	at org.apache.spark.SparkContext.getOrCreate(SparkContext.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at sparklyr.Invoke$.invoke(invoke.scala:94)
	at sparklyr.StreamHandler$.handleMethodCall(stream.scala:89)
	at sparklyr.StreamHandler$.read(stream.scala:55)
	at sparklyr.BackendHandler.channelRead0(handler.scala:49)
	at sparklyr.BackendHandler.channelRead0(handler.scala:14)
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:244)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137)
	at java.lang.Thread.run(Thread.java:745)
17/03/28 12:21:55 INFO SecurityManager: Changing view acls to: lifeng
17/03/28 12:21:55 INFO SecurityManager: Changing modify acls to: lifeng
17/03/28 12:21:55 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(lifeng); users with modify permissions: Set(lifeng)
17/03/28 12:21:55 INFO Utils: Successfully started service 'sparkDriver' on port 51009.
17/03/28 12:21:56 INFO Slf4jLogger: Slf4jLogger started
17/03/28 12:21:56 INFO Remoting: Starting remoting
17/03/28 12:21:56 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@127.0.0.1:51022]
17/03/28 12:21:56 INFO Utils: Successfully started service 'sparkDriverActorSystem' on port 51022.
17/03/28 12:21:56 INFO SparkEnv: Registering MapOutputTracker
17/03/28 12:21:56 INFO SparkEnv: Registering BlockManagerMaster
17/03/28 12:21:56 INFO DiskBlockManager: Created local directory at C:\Users\lifeng\AppData\Local\Temp\blockmgr-31f792ce-411d-4efc-853a-45b890399c11
17/03/28 12:21:56 INFO MemoryStore: MemoryStore started with capacity 511.1 MB
17/03/28 12:21:56 INFO SparkEnv: Registering OutputCommitCoordinator
17/03/28 12:21:56 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/03/28 12:21:56 INFO SparkUI: Started SparkUI at http://127.0.0.1:4040
17/03/28 12:21:56 INFO HttpFileServer: HTTP File server directory is C:\Users\lifeng\AppData\Local\Temp\spark-5d94a5ed-af23-44e3-be4c-df9a2877630e\httpd-7f080ccd-2adf-4683-bdf3-558247ddb791
17/03/28 12:21:56 INFO HttpServer: Starting HTTP Server
17/03/28 12:21:56 INFO Utils: Successfully started service 'HTTP file server' on port 51025.
17/03/28 12:21:56 INFO SparkContext: Added JAR file:/D:/R/R-3.3.3/library/sparklyr/java/spark-csv_2.11-1.3.0.jar at http://127.0.0.1:51025/jars/spark-csv_2.11-1.3.0.jar with timestamp 1490674916683
17/03/28 12:21:56 INFO SparkContext: Added JAR file:/D:/R/R-3.3.3/library/sparklyr/java/commons-csv-1.1.jar at http://127.0.0.1:51025/jars/commons-csv-1.1.jar with timestamp 1490674916685
17/03/28 12:21:56 INFO SparkContext: Added JAR file:/D:/R/R-3.3.3/library/sparklyr/java/univocity-parsers-1.5.1.jar at http://127.0.0.1:51025/jars/univocity-parsers-1.5.1.jar with timestamp 1490674916686
17/03/28 12:21:56 INFO SparkContext: Added JAR file:/D:/R/R-3.3.3/library/sparklyr/java/sparklyr-1.6-2.10.jar at http://127.0.0.1:51025/jars/sparklyr-1.6-2.10.jar with timestamp 1490674916687
17/03/28 12:21:56 INFO Executor: Starting executor ID driver on host localhost
17/03/28 12:21:56 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51042.
17/03/28 12:21:56 INFO NettyBlockTransferService: Server created on 51042
17/03/28 12:21:56 INFO BlockManagerMaster: Trying to register BlockManager
17/03/28 12:21:56 INFO BlockManagerMasterEndpoint: Registering block manager localhost:51042 with 511.1 MB RAM, BlockManagerId(driver, localhost, 51042)
17/03/28 12:21:56 INFO BlockManagerMaster: Registered BlockManager
17/03/28 12:22:12 INFO SparkContext: Invoking stop() from shutdown hook
17/03/28 12:22:12 INFO SparkUI: Stopped Spark web UI at http://127.0.0.1:4040
17/03/28 12:22:12 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
17/03/28 12:22:12 INFO MemoryStore: MemoryStore cleared
17/03/28 12:22:12 INFO BlockManager: BlockManager stopped
17/03/28 12:22:12 INFO BlockManagerMaster: BlockManagerMaster stopped
17/03/28 12:22:12 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
17/03/28 12:22:12 INFO SparkContext: Successfully stopped SparkContext
17/03/28 12:22:12 INFO ShutdownHookManager: Shutdown hook called
17/03/28 12:22:12 INFO ShutdownHookManager: Deleting directory C:\Users\lifeng\AppData\Local\Temp\spark-5d94a5ed-af23-44e3-be4c-df9a2877630e\httpd-7f080ccd-2adf-4683-bdf3-558247ddb791
17/03/28 12:22:12 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
17/03/28 12:22:12 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
17/03/28 12:22:12 INFO ShutdownHookManager: Deleting directory C:\Users\lifeng\AppData\Local\Temp\spark-5d94a5ed-af23-44e3-be4c-df9a2877630e
17/03/28 12:27:17 INFO SparkContext: Running Spark version 1.6.2
17/03/28 12:27:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/03/28 12:27:18 INFO SecurityManager: Changing view acls to: lifeng
17/03/28 12:27:18 INFO SecurityManager: Changing modify acls to: lifeng
17/03/28 12:27:18 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(lifeng); users with modify permissions: Set(lifeng)
17/03/28 12:27:18 INFO Utils: Successfully started service 'sparkDriver' on port 51199.
17/03/28 12:27:18 INFO Slf4jLogger: Slf4jLogger started
17/03/28 12:27:18 INFO Remoting: Starting remoting
17/03/28 12:27:18 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@127.0.0.1:51212]
17/03/28 12:27:18 INFO Utils: Successfully started service 'sparkDriverActorSystem' on port 51212.
17/03/28 12:27:18 INFO SparkEnv: Registering MapOutputTracker
17/03/28 12:27:18 INFO SparkEnv: Registering BlockManagerMaster
17/03/28 12:27:18 INFO DiskBlockManager: Created local directory at C:\Users\lifeng\AppData\Local\Temp\blockmgr-2e7643f4-b71d-4155-a068-192f38749efb
17/03/28 12:27:18 INFO MemoryStore: MemoryStore started with capacity 511.1 MB
17/03/28 12:27:18 INFO SparkEnv: Registering OutputCommitCoordinator
17/03/28 12:27:18 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/03/28 12:27:18 INFO SparkUI: Started SparkUI at http://127.0.0.1:4040
17/03/28 12:27:18 INFO HttpFileServer: HTTP File server directory is C:\Users\lifeng\AppData\Local\Temp\spark-979fa612-00aa-4809-a5e1-7b55a8c06d46\httpd-e817d45f-027d-4568-8f9b-d02d5923466a
17/03/28 12:27:18 INFO HttpServer: Starting HTTP Server
17/03/28 12:27:19 INFO Utils: Successfully started service 'HTTP file server' on port 51215.
17/03/28 12:27:19 INFO SparkContext: Added JAR file:/D:/R/R-3.3.3/library/sparklyr/java/spark-csv_2.11-1.3.0.jar at http://127.0.0.1:51215/jars/spark-csv_2.11-1.3.0.jar with timestamp 1490675239017
17/03/28 12:27:19 INFO SparkContext: Added JAR file:/D:/R/R-3.3.3/library/sparklyr/java/commons-csv-1.1.jar at http://127.0.0.1:51215/jars/commons-csv-1.1.jar with timestamp 1490675239019
17/03/28 12:27:19 INFO SparkContext: Added JAR file:/D:/R/R-3.3.3/library/sparklyr/java/univocity-parsers-1.5.1.jar at http://127.0.0.1:51215/jars/univocity-parsers-1.5.1.jar with timestamp 1490675239021
17/03/28 12:27:19 INFO SparkContext: Added JAR file:/D:/R/R-3.3.3/library/sparklyr/java/sparklyr-1.6-2.10.jar at http://127.0.0.1:51215/jars/sparklyr-1.6-2.10.jar with timestamp 1490675239024
17/03/28 12:27:19 INFO Executor: Starting executor ID driver on host localhost
17/03/28 12:27:19 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51232.
17/03/28 12:27:19 INFO NettyBlockTransferService: Server created on 51232
17/03/28 12:27:19 INFO BlockManagerMaster: Trying to register BlockManager
17/03/28 12:27:19 INFO BlockManagerMasterEndpoint: Registering block manager localhost:51232 with 511.1 MB RAM, BlockManagerId(driver, localhost, 51232)
17/03/28 12:27:19 INFO BlockManagerMaster: Registered BlockManager
17/03/28 12:27:19 INFO HiveContext: Initializing execution hive, version 1.2.1
17/03/28 12:27:19 INFO ClientWrapper: Inspected Hadoop version: 2.6.0
17/03/28 12:27:19 INFO ClientWrapper: Loaded org.apache.hadoop.hive.shims.Hadoop23Shims for Hadoop version 2.6.0
17/03/28 12:27:20 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/03/28 12:27:20 INFO ObjectStore: ObjectStore, initialize called
17/03/28 12:27:20 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
17/03/28 12:27:20 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
17/03/28 12:27:20 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
17/03/28 12:27:20 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
17/03/28 12:27:27 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
17/03/28 12:27:28 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 12:27:28 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 12:27:33 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 12:27:33 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 12:27:35 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
17/03/28 12:27:35 INFO ObjectStore: Initialized ObjectStore
17/03/28 12:27:35 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
17/03/28 12:27:35 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
17/03/28 12:27:45 INFO HiveMetaStore: Added admin role in metastore
17/03/28 12:27:45 INFO HiveMetaStore: Added public role in metastore
17/03/28 12:27:45 INFO HiveMetaStore: No user is added in admin role, since config is empty
17/03/28 12:27:45 INFO HiveMetaStore: 0: get_all_databases
17/03/28 12:27:45 INFO audit: ugi=lifeng	ip=unknown-ip-addr	cmd=get_all_databases	
17/03/28 12:27:45 INFO HiveMetaStore: 0: get_functions: db=default pat=*
17/03/28 12:27:45 INFO audit: ugi=lifeng	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
17/03/28 12:27:45 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 12:27:46 INFO SessionState: Created HDFS directory: D:/Spark/spark-1.6.2-bin-hadoop2.6/tmp/hive/lifeng
17/03/28 12:27:46 INFO SessionState: Created local directory: C:/Users/lifeng/AppData/Local/Temp/ebe5b0f6-c05b-4de3-816d-df5689bce7d7_resources
17/03/28 12:27:47 INFO SessionState: Created HDFS directory: D:/Spark/spark-1.6.2-bin-hadoop2.6/tmp/hive/lifeng/ebe5b0f6-c05b-4de3-816d-df5689bce7d7
17/03/28 12:27:47 INFO SessionState: Created local directory: D:/Spark/spark-1.6.2-bin-hadoop2.6/tmp/hive/ebe5b0f6-c05b-4de3-816d-df5689bce7d7
17/03/28 12:27:47 INFO SessionState: Created HDFS directory: D:/Spark/spark-1.6.2-bin-hadoop2.6/tmp/hive/lifeng/ebe5b0f6-c05b-4de3-816d-df5689bce7d7/_tmp_space.db
17/03/28 12:27:47 INFO HiveContext: default warehouse location is D:\Spark\spark-1.6.2-bin-hadoop2.6\tmp\hive
17/03/28 12:27:47 INFO HiveContext: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
17/03/28 12:27:47 INFO ClientWrapper: Inspected Hadoop version: 2.6.0
17/03/28 12:27:47 INFO ClientWrapper: Loaded org.apache.hadoop.hive.shims.Hadoop23Shims for Hadoop version 2.6.0
17/03/28 12:27:47 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/03/28 12:27:47 INFO ObjectStore: ObjectStore, initialize called
17/03/28 12:27:47 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
17/03/28 12:27:47 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
17/03/28 12:27:47 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
17/03/28 12:27:47 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
17/03/28 12:27:48 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
17/03/28 12:27:49 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 12:27:49 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 12:27:50 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 12:27:50 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 12:27:50 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
17/03/28 12:27:50 INFO ObjectStore: Initialized ObjectStore
17/03/28 12:27:50 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
17/03/28 12:27:50 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
17/03/28 12:27:50 INFO HiveMetaStore: Added admin role in metastore
17/03/28 12:27:50 INFO HiveMetaStore: Added public role in metastore
17/03/28 12:27:50 INFO HiveMetaStore: No user is added in admin role, since config is empty
17/03/28 12:27:50 INFO HiveMetaStore: 0: get_all_databases
17/03/28 12:27:50 INFO audit: ugi=lifeng	ip=unknown-ip-addr	cmd=get_all_databases	
17/03/28 12:27:50 INFO HiveMetaStore: 0: get_functions: db=default pat=*
17/03/28 12:27:50 INFO audit: ugi=lifeng	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
17/03/28 12:27:50 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 12:27:50 INFO SessionState: Created local directory: C:/Users/lifeng/AppData/Local/Temp/6ca55748-2fac-4780-9bde-486073f1d318_resources
17/03/28 12:27:50 INFO SessionState: Created HDFS directory: D:/Spark/spark-1.6.2-bin-hadoop2.6/tmp/hive/lifeng/6ca55748-2fac-4780-9bde-486073f1d318
17/03/28 12:27:50 INFO SessionState: Created local directory: D:/Spark/spark-1.6.2-bin-hadoop2.6/tmp/hive/6ca55748-2fac-4780-9bde-486073f1d318
17/03/28 12:27:51 INFO SessionState: Created HDFS directory: D:/Spark/spark-1.6.2-bin-hadoop2.6/tmp/hive/lifeng/6ca55748-2fac-4780-9bde-486073f1d318/_tmp_space.db
17/03/28 12:27:51 INFO HiveMetaStore: 0: get_tables: db=default pat=.*
17/03/28 12:27:51 INFO audit: ugi=lifeng	ip=unknown-ip-addr	cmd=get_tables: db=default pat=.*	
17/03/28 12:27:51 INFO SparkContext: Starting job: collect at utils.scala:59
17/03/28 12:27:51 INFO DAGScheduler: Got job 0 (collect at utils.scala:59) with 1 output partitions
17/03/28 12:27:51 INFO DAGScheduler: Final stage: ResultStage 0 (collect at utils.scala:59)
17/03/28 12:27:51 INFO DAGScheduler: Parents of final stage: List()
17/03/28 12:27:51 INFO DAGScheduler: Missing parents: List()
17/03/28 12:27:51 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at map at utils.scala:56), which has no missing parents
17/03/28 12:27:51 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 5.4 KB, free 5.4 KB)
17/03/28 12:27:51 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.0 KB, free 8.4 KB)
17/03/28 12:27:51 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:51232 (size: 3.0 KB, free: 511.1 MB)
17/03/28 12:27:52 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1006
17/03/28 12:27:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at map at utils.scala:56)
17/03/28 12:27:52 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
17/03/28 12:27:52 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2348 bytes)
17/03/28 12:27:52 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
17/03/28 12:27:52 INFO Executor: Fetching http://127.0.0.1:51215/jars/univocity-parsers-1.5.1.jar with timestamp 1490675239021
17/03/28 12:27:52 INFO Utils: Fetching http://127.0.0.1:51215/jars/univocity-parsers-1.5.1.jar to C:\Users\lifeng\AppData\Local\Temp\spark-979fa612-00aa-4809-a5e1-7b55a8c06d46\userFiles-4aa4fd06-cec6-4958-b41b-31ce0da1a60d\fetchFileTemp8306613037562375386.tmp
17/03/28 12:27:52 INFO Executor: Adding file:/C:/Users/lifeng/AppData/Local/Temp/spark-979fa612-00aa-4809-a5e1-7b55a8c06d46/userFiles-4aa4fd06-cec6-4958-b41b-31ce0da1a60d/univocity-parsers-1.5.1.jar to class loader
17/03/28 12:27:52 INFO Executor: Fetching http://127.0.0.1:51215/jars/sparklyr-1.6-2.10.jar with timestamp 1490675239024
17/03/28 12:27:52 INFO Utils: Fetching http://127.0.0.1:51215/jars/sparklyr-1.6-2.10.jar to C:\Users\lifeng\AppData\Local\Temp\spark-979fa612-00aa-4809-a5e1-7b55a8c06d46\userFiles-4aa4fd06-cec6-4958-b41b-31ce0da1a60d\fetchFileTemp5497446529766225118.tmp
17/03/28 12:27:52 INFO Executor: Adding file:/C:/Users/lifeng/AppData/Local/Temp/spark-979fa612-00aa-4809-a5e1-7b55a8c06d46/userFiles-4aa4fd06-cec6-4958-b41b-31ce0da1a60d/sparklyr-1.6-2.10.jar to class loader
17/03/28 12:27:52 INFO Executor: Fetching http://127.0.0.1:51215/jars/spark-csv_2.11-1.3.0.jar with timestamp 1490675239017
17/03/28 12:27:52 INFO Utils: Fetching http://127.0.0.1:51215/jars/spark-csv_2.11-1.3.0.jar to C:\Users\lifeng\AppData\Local\Temp\spark-979fa612-00aa-4809-a5e1-7b55a8c06d46\userFiles-4aa4fd06-cec6-4958-b41b-31ce0da1a60d\fetchFileTemp6498389251469268424.tmp
17/03/28 12:27:52 INFO Executor: Adding file:/C:/Users/lifeng/AppData/Local/Temp/spark-979fa612-00aa-4809-a5e1-7b55a8c06d46/userFiles-4aa4fd06-cec6-4958-b41b-31ce0da1a60d/spark-csv_2.11-1.3.0.jar to class loader
17/03/28 12:27:52 INFO Executor: Fetching http://127.0.0.1:51215/jars/commons-csv-1.1.jar with timestamp 1490675239019
17/03/28 12:27:52 INFO Utils: Fetching http://127.0.0.1:51215/jars/commons-csv-1.1.jar to C:\Users\lifeng\AppData\Local\Temp\spark-979fa612-00aa-4809-a5e1-7b55a8c06d46\userFiles-4aa4fd06-cec6-4958-b41b-31ce0da1a60d\fetchFileTemp7850727300680000423.tmp
17/03/28 12:27:52 INFO Executor: Adding file:/C:/Users/lifeng/AppData/Local/Temp/spark-979fa612-00aa-4809-a5e1-7b55a8c06d46/userFiles-4aa4fd06-cec6-4958-b41b-31ce0da1a60d/commons-csv-1.1.jar to class loader
17/03/28 12:27:52 INFO GenerateUnsafeProjection: Code generated in 125.830616 ms
17/03/28 12:27:52 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1060 bytes result sent to driver
17/03/28 12:27:52 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 722 ms on localhost (1/1)
17/03/28 12:27:52 INFO DAGScheduler: ResultStage 0 (collect at utils.scala:59) finished in 0.732 s
17/03/28 12:27:52 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
17/03/28 12:27:52 INFO DAGScheduler: Job 0 finished: collect at utils.scala:59, took 0.831685 s
17/03/28 12:27:52 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 61.8 KB, free 70.2 KB)
17/03/28 12:27:52 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 19.3 KB, free 89.5 KB)
17/03/28 12:27:52 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:51232 (size: 19.3 KB, free: 511.1 MB)
17/03/28 12:27:52 INFO SparkContext: Created broadcast 1 from textFile at TextFile.scala:30
17/03/28 12:27:52 INFO FileInputFormat: Total input paths to process : 1
17/03/28 12:27:52 INFO SparkContext: Starting job: take at CsvRelation.scala:249
17/03/28 12:27:52 INFO DAGScheduler: Got job 1 (take at CsvRelation.scala:249) with 1 output partitions
17/03/28 12:27:52 INFO DAGScheduler: Final stage: ResultStage 1 (take at CsvRelation.scala:249)
17/03/28 12:27:52 INFO DAGScheduler: Parents of final stage: List()
17/03/28 12:27:52 INFO DAGScheduler: Missing parents: List()
17/03/28 12:27:52 INFO DAGScheduler: Submitting ResultStage 1 (C:\Users\lifeng\AppData\Local\Temp\RtmpSm9XR3/spark_serialize_9fd0bf1861994b0d294634211269ec9e591b014b83a5683f179dd18e7e70ef0b.csv MapPartitionsRDD[5] at textFile at TextFile.scala:30), which has no missing parents
17/03/28 12:27:52 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 3.2 KB, free 92.7 KB)
17/03/28 12:27:52 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 1946.0 B, free 94.6 KB)
17/03/28 12:27:52 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:51232 (size: 1946.0 B, free: 511.1 MB)
17/03/28 12:27:52 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1006
17/03/28 12:27:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (C:\Users\lifeng\AppData\Local\Temp\RtmpSm9XR3/spark_serialize_9fd0bf1861994b0d294634211269ec9e591b014b83a5683f179dd18e7e70ef0b.csv MapPartitionsRDD[5] at textFile at TextFile.scala:30)
17/03/28 12:27:52 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
17/03/28 12:27:52 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2482 bytes)
17/03/28 12:27:52 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
17/03/28 12:27:53 INFO HadoopRDD: Input split: file:/C:/Users/lifeng/AppData/Local/Temp/RtmpSm9XR3/spark_serialize_9fd0bf1861994b0d294634211269ec9e591b014b83a5683f179dd18e7e70ef0b.csv:0+2088
17/03/28 12:27:53 INFO deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id
17/03/28 12:27:53 INFO deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
17/03/28 12:27:53 INFO deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
17/03/28 12:27:53 INFO deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
17/03/28 12:27:53 INFO deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id
17/03/28 12:27:53 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2354 bytes result sent to driver
17/03/28 12:27:53 INFO DAGScheduler: ResultStage 1 (take at CsvRelation.scala:249) finished in 0.050 s
17/03/28 12:27:53 INFO DAGScheduler: Job 1 finished: take at CsvRelation.scala:249, took 0.059269 s
17/03/28 12:27:53 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 50 ms on localhost (1/1)
17/03/28 12:27:53 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
17/03/28 12:27:53 INFO ParseDriver: Parsing command: SELECT * FROM  `iris`
17/03/28 12:27:53 INFO ParseDriver: Parse Completed
17/03/28 12:27:53 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 208.5 KB, free 303.1 KB)
17/03/28 12:27:53 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 19.3 KB, free 322.5 KB)
17/03/28 12:27:53 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on localhost:51232 (size: 19.3 KB, free: 511.1 MB)
17/03/28 12:27:53 INFO SparkContext: Created broadcast 3 from textFile at TextFile.scala:30
17/03/28 12:27:53 INFO FileInputFormat: Total input paths to process : 1
17/03/28 12:27:53 INFO SparkContext: Starting job: take at CsvRelation.scala:249
17/03/28 12:27:53 INFO DAGScheduler: Got job 2 (take at CsvRelation.scala:249) with 1 output partitions
17/03/28 12:27:53 INFO DAGScheduler: Final stage: ResultStage 2 (take at CsvRelation.scala:249)
17/03/28 12:27:53 INFO DAGScheduler: Parents of final stage: List()
17/03/28 12:27:53 INFO DAGScheduler: Missing parents: List()
17/03/28 12:27:53 INFO DAGScheduler: Submitting ResultStage 2 (C:\Users\lifeng\AppData\Local\Temp\RtmpSm9XR3/spark_serialize_9fd0bf1861994b0d294634211269ec9e591b014b83a5683f179dd18e7e70ef0b.csv MapPartitionsRDD[7] at textFile at TextFile.scala:30), which has no missing parents
17/03/28 12:27:53 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 3.2 KB, free 325.7 KB)
17/03/28 12:27:53 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 1947.0 B, free 327.6 KB)
17/03/28 12:27:53 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on localhost:51232 (size: 1947.0 B, free: 511.1 MB)
17/03/28 12:27:53 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1006
17/03/28 12:27:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (C:\Users\lifeng\AppData\Local\Temp\RtmpSm9XR3/spark_serialize_9fd0bf1861994b0d294634211269ec9e591b014b83a5683f179dd18e7e70ef0b.csv MapPartitionsRDD[7] at textFile at TextFile.scala:30)
17/03/28 12:27:53 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
17/03/28 12:27:53 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0,PROCESS_LOCAL, 2482 bytes)
17/03/28 12:27:53 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
17/03/28 12:27:53 INFO HadoopRDD: Input split: file:/C:/Users/lifeng/AppData/Local/Temp/RtmpSm9XR3/spark_serialize_9fd0bf1861994b0d294634211269ec9e591b014b83a5683f179dd18e7e70ef0b.csv:0+2088
17/03/28 12:27:53 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2354 bytes result sent to driver
17/03/28 12:27:53 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 7 ms on localhost (1/1)
17/03/28 12:27:53 INFO DAGScheduler: ResultStage 2 (take at CsvRelation.scala:249) finished in 0.008 s
17/03/28 12:27:53 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
17/03/28 12:27:53 INFO DAGScheduler: Job 2 finished: take at CsvRelation.scala:249, took 0.015251 s
17/03/28 12:27:53 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 208.5 KB, free 536.1 KB)
17/03/28 12:27:53 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 19.3 KB, free 555.4 KB)
17/03/28 12:27:53 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on localhost:51232 (size: 19.3 KB, free: 511.1 MB)
17/03/28 12:27:53 INFO SparkContext: Created broadcast 5 from textFile at TextFile.scala:30
17/03/28 12:27:53 INFO FileInputFormat: Total input paths to process : 1
17/03/28 12:27:54 INFO SparkContext: Starting job: sql at NativeMethodAccessorImpl.java:-2
17/03/28 12:27:54 INFO DAGScheduler: Registering RDD 17 (sql at NativeMethodAccessorImpl.java:-2)
17/03/28 12:27:54 INFO DAGScheduler: Got job 3 (sql at NativeMethodAccessorImpl.java:-2) with 1 output partitions
17/03/28 12:27:54 INFO DAGScheduler: Final stage: ResultStage 4 (sql at NativeMethodAccessorImpl.java:-2)
17/03/28 12:27:54 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
17/03/28 12:27:54 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 3)
17/03/28 12:27:54 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[17] at sql at NativeMethodAccessorImpl.java:-2), which has no missing parents
17/03/28 12:27:54 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 18.3 KB, free 573.7 KB)
17/03/28 12:27:54 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 8.7 KB, free 582.3 KB)
17/03/28 12:27:54 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on localhost:51232 (size: 8.7 KB, free: 511.1 MB)
17/03/28 12:27:54 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1006
17/03/28 12:27:54 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[17] at sql at NativeMethodAccessorImpl.java:-2)
17/03/28 12:27:54 INFO TaskSchedulerImpl: Adding task set 3.0 with 2 tasks
17/03/28 12:27:54 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, localhost, partition 0,PROCESS_LOCAL, 2471 bytes)
17/03/28 12:27:54 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 4, localhost, partition 1,PROCESS_LOCAL, 2471 bytes)
17/03/28 12:27:54 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
17/03/28 12:27:54 INFO Executor: Running task 1.0 in stage 3.0 (TID 4)
17/03/28 12:27:54 INFO CacheManager: Partition rdd_14_0 not found, computing it
17/03/28 12:27:54 INFO HadoopRDD: Input split: file:/C:/Users/lifeng/AppData/Local/Temp/RtmpSm9XR3/spark_serialize_9fd0bf1861994b0d294634211269ec9e591b014b83a5683f179dd18e7e70ef0b.csv:0+2088
17/03/28 12:27:54 INFO CacheManager: Partition rdd_14_1 not found, computing it
17/03/28 12:27:54 INFO HadoopRDD: Input split: file:/C:/Users/lifeng/AppData/Local/Temp/RtmpSm9XR3/spark_serialize_9fd0bf1861994b0d294634211269ec9e591b014b83a5683f179dd18e7e70ef0b.csv:2088+2089
17/03/28 12:27:54 INFO GenerateUnsafeProjection: Code generated in 12.837521 ms
17/03/28 12:27:54 INFO MemoryStore: Block rdd_14_0 stored as values in memory (estimated size 3.2 KB, free 585.5 KB)
17/03/28 12:27:54 INFO MemoryStore: Block rdd_14_1 stored as values in memory (estimated size 3.1 KB, free 588.7 KB)
17/03/28 12:27:54 INFO BlockManagerInfo: Added rdd_14_0 in memory on localhost:51232 (size: 3.2 KB, free: 511.1 MB)
17/03/28 12:27:54 INFO BlockManagerInfo: Added rdd_14_1 in memory on localhost:51232 (size: 3.1 KB, free: 511.0 MB)
17/03/28 12:27:54 INFO GeneratePredicate: Code generated in 5.271699 ms
17/03/28 12:27:54 INFO GenerateColumnAccessor: Code generated in 16.045024 ms
17/03/28 12:27:54 INFO GenerateMutableProjection: Code generated in 7.59229 ms
17/03/28 12:27:54 INFO GenerateUnsafeProjection: Code generated in 5.701132 ms
17/03/28 12:27:54 INFO GenerateMutableProjection: Code generated in 8.639993 ms
17/03/28 12:27:54 INFO GenerateUnsafeRowJoiner: Code generated in 6.906859 ms
17/03/28 12:27:54 INFO GenerateUnsafeProjection: Code generated in 8.62103 ms
17/03/28 12:27:54 INFO Executor: Finished task 1.0 in stage 3.0 (TID 4). 3787 bytes result sent to driver
17/03/28 12:27:54 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 3784 bytes result sent to driver
17/03/28 12:27:54 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 301 ms on localhost (1/2)
17/03/28 12:27:54 INFO DAGScheduler: ShuffleMapStage 3 (sql at NativeMethodAccessorImpl.java:-2) finished in 0.302 s
17/03/28 12:27:54 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 4) in 301 ms on localhost (2/2)
17/03/28 12:27:54 INFO DAGScheduler: looking for newly runnable stages
17/03/28 12:27:54 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
17/03/28 12:27:54 INFO DAGScheduler: running: Set()
17/03/28 12:27:54 INFO DAGScheduler: waiting: Set(ResultStage 4)
17/03/28 12:27:54 INFO DAGScheduler: failed: Set()
17/03/28 12:27:54 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[20] at sql at NativeMethodAccessorImpl.java:-2), which has no missing parents
17/03/28 12:27:54 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 9.3 KB, free 597.9 KB)
17/03/28 12:27:54 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 4.6 KB, free 602.6 KB)
17/03/28 12:27:54 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:51232 (size: 4.6 KB, free: 511.0 MB)
17/03/28 12:27:54 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1006
17/03/28 12:27:54 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[20] at sql at NativeMethodAccessorImpl.java:-2)
17/03/28 12:27:54 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks
17/03/28 12:27:54 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 5, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/03/28 12:27:54 INFO Executor: Running task 0.0 in stage 4.0 (TID 5)
17/03/28 12:27:54 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/03/28 12:27:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
17/03/28 12:27:54 INFO GenerateMutableProjection: Code generated in 6.422909 ms
17/03/28 12:27:54 INFO GenerateMutableProjection: Code generated in 5.970168 ms
17/03/28 12:27:54 INFO Executor: Finished task 0.0 in stage 4.0 (TID 5). 1830 bytes result sent to driver
17/03/28 12:27:54 INFO DAGScheduler: ResultStage 4 (sql at NativeMethodAccessorImpl.java:-2) finished in 0.092 s
17/03/28 12:27:54 INFO DAGScheduler: Job 3 finished: sql at NativeMethodAccessorImpl.java:-2, took 0.436681 s
17/03/28 12:27:54 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 5) in 91 ms on localhost (1/1)
17/03/28 12:27:54 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
17/03/28 12:27:54 INFO ParseDriver: Parsing command: SELECT count(*) FROM  `iris`
17/03/28 12:27:54 INFO ParseDriver: Parse Completed
17/03/28 12:27:54 INFO SparkContext: Starting job: collect at utils.scala:195
17/03/28 12:27:54 INFO DAGScheduler: Registering RDD 24 (collect at utils.scala:195)
17/03/28 12:27:54 INFO DAGScheduler: Got job 4 (collect at utils.scala:195) with 1 output partitions
17/03/28 12:27:54 INFO DAGScheduler: Final stage: ResultStage 6 (collect at utils.scala:195)
17/03/28 12:27:54 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)
17/03/28 12:27:54 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 5)
17/03/28 12:27:54 INFO DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[24] at collect at utils.scala:195), which has no missing parents
17/03/28 12:27:54 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 18.4 KB, free 620.9 KB)
17/03/28 12:27:54 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 8.7 KB, free 629.6 KB)
17/03/28 12:27:54 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on localhost:51232 (size: 8.7 KB, free: 511.0 MB)
17/03/28 12:27:54 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1006
17/03/28 12:27:54 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[24] at collect at utils.scala:195)
17/03/28 12:27:54 INFO TaskSchedulerImpl: Adding task set 5.0 with 2 tasks
17/03/28 12:27:54 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 6, localhost, partition 0,PROCESS_LOCAL, 2471 bytes)
17/03/28 12:27:54 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 7, localhost, partition 1,PROCESS_LOCAL, 2471 bytes)
17/03/28 12:27:54 INFO Executor: Running task 0.0 in stage 5.0 (TID 6)
17/03/28 12:27:54 INFO Executor: Running task 1.0 in stage 5.0 (TID 7)
17/03/28 12:27:54 INFO BlockManager: Found block rdd_14_1 locally
17/03/28 12:27:54 INFO BlockManager: Found block rdd_14_0 locally
17/03/28 12:27:54 INFO Executor: Finished task 1.0 in stage 5.0 (TID 7). 2679 bytes result sent to driver
17/03/28 12:27:54 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 7) in 27 ms on localhost (1/2)
17/03/28 12:27:54 INFO Executor: Finished task 0.0 in stage 5.0 (TID 6). 2679 bytes result sent to driver
17/03/28 12:27:54 INFO DAGScheduler: ShuffleMapStage 5 (collect at utils.scala:195) finished in 0.034 s
17/03/28 12:27:54 INFO DAGScheduler: looking for newly runnable stages
17/03/28 12:27:54 INFO DAGScheduler: running: Set()
17/03/28 12:27:54 INFO DAGScheduler: waiting: Set(ResultStage 6)
17/03/28 12:27:54 INFO DAGScheduler: failed: Set()
17/03/28 12:27:54 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[27] at collect at utils.scala:195), which has no missing parents
17/03/28 12:27:54 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 9.4 KB, free 639.0 KB)
17/03/28 12:27:54 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 4.6 KB, free 643.6 KB)
17/03/28 12:27:54 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 6) in 33 ms on localhost (2/2)
17/03/28 12:27:54 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
17/03/28 12:27:54 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on localhost:51232 (size: 4.6 KB, free: 511.0 MB)
17/03/28 12:27:54 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1006
17/03/28 12:27:54 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[27] at collect at utils.scala:195)
17/03/28 12:27:54 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks
17/03/28 12:27:54 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 8, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/03/28 12:27:54 INFO Executor: Running task 0.0 in stage 6.0 (TID 8)
17/03/28 12:27:54 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/03/28 12:27:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/03/28 12:27:54 INFO Executor: Finished task 0.0 in stage 6.0 (TID 8). 1830 bytes result sent to driver
17/03/28 12:27:54 INFO DAGScheduler: ResultStage 6 (collect at utils.scala:195) finished in 0.009 s
17/03/28 12:27:54 INFO DAGScheduler: Job 4 finished: collect at utils.scala:195, took 0.062716 s
17/03/28 12:27:54 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 8) in 9 ms on localhost (1/1)
17/03/28 12:27:54 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
17/03/28 12:27:54 INFO ParseDriver: Parsing command: SELECT *
FROM `iris` AS `zzz1`
WHERE (0 = 1)
17/03/28 12:27:54 INFO ParseDriver: Parse Completed
17/03/28 12:27:54 INFO HiveMetaStore: 0: get_tables: db=default pat=.*
17/03/28 12:27:54 INFO audit: ugi=lifeng	ip=unknown-ip-addr	cmd=get_tables: db=default pat=.*	
17/03/28 12:27:54 INFO ContextCleaner: Cleaned accumulator 2
17/03/28 12:27:54 INFO ContextCleaner: Cleaned accumulator 10
17/03/28 12:27:54 INFO ContextCleaner: Cleaned accumulator 9
17/03/28 12:27:54 INFO ContextCleaner: Cleaned accumulator 8
17/03/28 12:27:54 INFO ContextCleaner: Cleaned accumulator 7
17/03/28 12:27:54 INFO SparkContext: Starting job: collect at utils.scala:195
17/03/28 12:27:54 INFO DAGScheduler: Got job 5 (collect at utils.scala:195) with 1 output partitions
17/03/28 12:27:54 INFO DAGScheduler: Final stage: ResultStage 7 (collect at utils.scala:195)
17/03/28 12:27:54 INFO DAGScheduler: Parents of final stage: List()
17/03/28 12:27:54 INFO DAGScheduler: Missing parents: List()
17/03/28 12:27:54 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[29] at collect at utils.scala:195), which has no missing parents
17/03/28 12:27:54 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 1968.0 B, free 645.5 KB)
17/03/28 12:27:54 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 1224.0 B, free 646.7 KB)
17/03/28 12:27:54 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on localhost:51232 (size: 1224.0 B, free: 511.0 MB)
17/03/28 12:27:54 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1006
17/03/28 12:27:54 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[29] at collect at utils.scala:195)
17/03/28 12:27:54 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks
17/03/28 12:27:54 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 9, localhost, partition 0,PROCESS_LOCAL, 2646 bytes)
17/03/28 12:27:54 INFO Executor: Running task 0.0 in stage 7.0 (TID 9)
17/03/28 12:27:54 INFO Executor: Finished task 0.0 in stage 7.0 (TID 9). 1258 bytes result sent to driver
17/03/28 12:27:54 INFO DAGScheduler: ResultStage 7 (collect at utils.scala:195) finished in 0.007 s
17/03/28 12:27:54 INFO DAGScheduler: Job 5 finished: collect at utils.scala:195, took 0.017943 s
17/03/28 12:27:54 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 9) in 7 ms on localhost (1/1)
17/03/28 12:27:54 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
17/03/28 12:27:54 INFO BlockManagerInfo: Removed broadcast_4_piece0 on localhost:51232 in memory (size: 1947.0 B, free: 511.0 MB)
17/03/28 12:27:54 INFO ContextCleaner: Cleaned accumulator 5
17/03/28 12:27:54 INFO BlockManagerInfo: Removed broadcast_3_piece0 on localhost:51232 in memory (size: 19.3 KB, free: 511.0 MB)
17/03/28 12:27:54 INFO BlockManagerInfo: Removed broadcast_2_piece0 on localhost:51232 in memory (size: 1946.0 B, free: 511.1 MB)
17/03/28 12:27:54 INFO ContextCleaner: Cleaned accumulator 4
17/03/28 12:27:54 INFO BlockManagerInfo: Removed broadcast_1_piece0 on localhost:51232 in memory (size: 19.3 KB, free: 511.1 MB)
17/03/28 12:27:54 INFO BlockManagerInfo: Removed broadcast_0_piece0 on localhost:51232 in memory (size: 3.0 KB, free: 511.1 MB)
17/03/28 12:27:54 INFO ContextCleaner: Cleaned accumulator 3
17/03/28 12:27:54 INFO BlockManagerInfo: Removed broadcast_9_piece0 on localhost:51232 in memory (size: 4.6 KB, free: 511.1 MB)
17/03/28 12:27:54 INFO ContextCleaner: Cleaned accumulator 26
17/03/28 12:27:54 INFO BlockManagerInfo: Removed broadcast_8_piece0 on localhost:51232 in memory (size: 8.7 KB, free: 511.1 MB)
17/03/28 12:27:54 INFO ContextCleaner: Cleaned accumulator 25
17/03/28 12:27:54 INFO ContextCleaner: Cleaned shuffle 1
17/03/28 12:27:54 INFO BlockManagerInfo: Removed broadcast_7_piece0 on localhost:51232 in memory (size: 4.6 KB, free: 511.1 MB)
17/03/28 12:27:54 INFO ContextCleaner: Cleaned accumulator 16
17/03/28 12:27:54 INFO BlockManagerInfo: Removed broadcast_6_piece0 on localhost:51232 in memory (size: 8.7 KB, free: 511.1 MB)
17/03/28 12:27:54 INFO ContextCleaner: Cleaned accumulator 15
17/03/28 12:27:54 INFO ContextCleaner: Cleaned shuffle 0
17/03/28 12:27:54 INFO ContextCleaner: Cleaned accumulator 14
17/03/28 12:27:54 INFO ContextCleaner: Cleaned accumulator 13
17/03/28 12:27:54 INFO ContextCleaner: Cleaned accumulator 12
17/03/28 12:27:54 INFO ContextCleaner: Cleaned accumulator 11
17/03/28 12:29:35 INFO SparkContext: Invoking stop() from shutdown hook
17/03/28 12:29:35 INFO SparkUI: Stopped Spark web UI at http://127.0.0.1:4040
17/03/28 12:29:35 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
17/03/28 12:29:35 INFO MemoryStore: MemoryStore cleared
17/03/28 12:29:35 INFO BlockManager: BlockManager stopped
17/03/28 12:29:35 INFO BlockManagerMaster: BlockManagerMaster stopped
17/03/28 12:29:35 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
17/03/28 12:29:35 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
17/03/28 12:29:35 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
17/03/28 12:29:35 WARN SparkEnv: Exception while deleting Spark temp dir: C:\Users\lifeng\AppData\Local\Temp\spark-979fa612-00aa-4809-a5e1-7b55a8c06d46\userFiles-4aa4fd06-cec6-4958-b41b-31ce0da1a60d
java.io.IOException: Failed to delete: C:\Users\lifeng\AppData\Local\Temp\spark-979fa612-00aa-4809-a5e1-7b55a8c06d46\userFiles-4aa4fd06-cec6-4958-b41b-31ce0da1a60d
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:929)
	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:119)
	at org.apache.spark.SparkContext$$anonfun$stop$12.apply$mcV$sp(SparkContext.scala:1756)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1219)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1755)
	at org.apache.spark.SparkContext$$anonfun$3.apply$mcV$sp(SparkContext.scala:596)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:267)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1801)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:218)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
17/03/28 12:29:35 INFO SparkContext: Successfully stopped SparkContext
17/03/28 12:29:35 INFO ShutdownHookManager: Shutdown hook called
17/03/28 12:29:35 INFO ShutdownHookManager: Deleting directory C:\Users\lifeng\AppData\Local\Temp\spark-86406c9b-26f7-4655-8564-aa1c4f6e283a
17/03/28 12:29:35 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.
17/03/28 12:29:35 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\lifeng\AppData\Local\Temp\spark-86406c9b-26f7-4655-8564-aa1c4f6e283a
java.io.IOException: Failed to delete: C:\Users\lifeng\AppData\Local\Temp\spark-86406c9b-26f7-4655-8564-aa1c4f6e283a
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:929)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:267)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1801)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:218)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
17/03/28 12:29:35 INFO ShutdownHookManager: Deleting directory C:\Users\lifeng\AppData\Local\Temp\spark-979fa612-00aa-4809-a5e1-7b55a8c06d46\userFiles-4aa4fd06-cec6-4958-b41b-31ce0da1a60d
17/03/28 12:29:35 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\lifeng\AppData\Local\Temp\spark-979fa612-00aa-4809-a5e1-7b55a8c06d46\userFiles-4aa4fd06-cec6-4958-b41b-31ce0da1a60d
java.io.IOException: Failed to delete: C:\Users\lifeng\AppData\Local\Temp\spark-979fa612-00aa-4809-a5e1-7b55a8c06d46\userFiles-4aa4fd06-cec6-4958-b41b-31ce0da1a60d
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:929)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:267)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1801)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:218)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
17/03/28 12:29:35 INFO ShutdownHookManager: Deleting directory C:\Users\lifeng\AppData\Local\Temp\spark-979fa612-00aa-4809-a5e1-7b55a8c06d46\httpd-e817d45f-027d-4568-8f9b-d02d5923466a
17/03/28 12:29:35 INFO ShutdownHookManager: Deleting directory C:\Users\lifeng\AppData\Local\Temp\spark-979fa612-00aa-4809-a5e1-7b55a8c06d46
17/03/28 12:29:35 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\lifeng\AppData\Local\Temp\spark-979fa612-00aa-4809-a5e1-7b55a8c06d46
java.io.IOException: Failed to delete: C:\Users\lifeng\AppData\Local\Temp\spark-979fa612-00aa-4809-a5e1-7b55a8c06d46
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:929)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:267)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1801)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:218)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
17/03/28 12:30:44 INFO SparkContext: Running Spark version 1.6.2
17/03/28 12:30:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/03/28 12:30:44 INFO SecurityManager: Changing view acls to: lifeng
17/03/28 12:30:44 INFO SecurityManager: Changing modify acls to: lifeng
17/03/28 12:30:44 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(lifeng); users with modify permissions: Set(lifeng)
17/03/28 12:30:45 INFO Utils: Successfully started service 'sparkDriver' on port 51398.
17/03/28 12:30:45 INFO Slf4jLogger: Slf4jLogger started
17/03/28 12:30:45 INFO Remoting: Starting remoting
17/03/28 12:30:45 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@127.0.0.1:51411]
17/03/28 12:30:45 INFO Utils: Successfully started service 'sparkDriverActorSystem' on port 51411.
17/03/28 12:30:45 INFO SparkEnv: Registering MapOutputTracker
17/03/28 12:30:45 INFO SparkEnv: Registering BlockManagerMaster
17/03/28 12:30:45 INFO DiskBlockManager: Created local directory at C:\Users\lifeng\AppData\Local\Temp\blockmgr-4f206b6f-222e-4264-83b7-e190472b6ec5
17/03/28 12:30:45 INFO MemoryStore: MemoryStore started with capacity 511.1 MB
17/03/28 12:30:45 INFO SparkEnv: Registering OutputCommitCoordinator
17/03/28 12:30:45 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/03/28 12:30:45 INFO SparkUI: Started SparkUI at http://127.0.0.1:4040
17/03/28 12:30:45 INFO HttpFileServer: HTTP File server directory is C:\Users\lifeng\AppData\Local\Temp\spark-30a2418b-c83a-4d4d-86af-0c5d2397bf7f\httpd-d9498df8-9833-4a70-85ea-d7d34a1fdd0d
17/03/28 12:30:45 INFO HttpServer: Starting HTTP Server
17/03/28 12:30:45 INFO Utils: Successfully started service 'HTTP file server' on port 51414.
17/03/28 12:30:45 INFO SparkContext: Added JAR file:/D:/R/R-3.3.3/library/sparklyr/java/spark-csv_2.11-1.3.0.jar at http://127.0.0.1:51414/jars/spark-csv_2.11-1.3.0.jar with timestamp 1490675445922
17/03/28 12:30:45 INFO SparkContext: Added JAR file:/D:/R/R-3.3.3/library/sparklyr/java/commons-csv-1.1.jar at http://127.0.0.1:51414/jars/commons-csv-1.1.jar with timestamp 1490675445924
17/03/28 12:30:45 INFO SparkContext: Added JAR file:/D:/R/R-3.3.3/library/sparklyr/java/univocity-parsers-1.5.1.jar at http://127.0.0.1:51414/jars/univocity-parsers-1.5.1.jar with timestamp 1490675445926
17/03/28 12:30:45 INFO SparkContext: Added JAR file:/D:/R/R-3.3.3/library/sparklyr/java/sparklyr-1.6-2.10.jar at http://127.0.0.1:51414/jars/sparklyr-1.6-2.10.jar with timestamp 1490675445927
17/03/28 12:30:45 INFO Executor: Starting executor ID driver on host localhost
17/03/28 12:30:45 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51431.
17/03/28 12:30:45 INFO NettyBlockTransferService: Server created on 51431
17/03/28 12:30:45 INFO BlockManagerMaster: Trying to register BlockManager
17/03/28 12:30:45 INFO BlockManagerMasterEndpoint: Registering block manager localhost:51431 with 511.1 MB RAM, BlockManagerId(driver, localhost, 51431)
17/03/28 12:30:46 INFO BlockManagerMaster: Registered BlockManager
17/03/28 12:30:46 INFO HiveContext: Initializing execution hive, version 1.2.1
17/03/28 12:30:46 INFO ClientWrapper: Inspected Hadoop version: 2.6.0
17/03/28 12:30:46 INFO ClientWrapper: Loaded org.apache.hadoop.hive.shims.Hadoop23Shims for Hadoop version 2.6.0
17/03/28 12:30:47 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/03/28 12:30:47 INFO ObjectStore: ObjectStore, initialize called
17/03/28 12:30:47 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
17/03/28 12:30:47 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
17/03/28 12:30:47 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
17/03/28 12:30:47 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
17/03/28 12:30:54 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
17/03/28 12:30:55 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 12:30:55 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 12:31:01 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 12:31:01 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 12:31:02 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
17/03/28 12:31:02 INFO ObjectStore: Initialized ObjectStore
17/03/28 12:31:02 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
17/03/28 12:31:03 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
17/03/28 12:31:12 INFO HiveMetaStore: Added admin role in metastore
17/03/28 12:31:12 INFO HiveMetaStore: Added public role in metastore
17/03/28 12:31:12 INFO HiveMetaStore: No user is added in admin role, since config is empty
17/03/28 12:31:12 INFO HiveMetaStore: 0: get_all_databases
17/03/28 12:31:12 INFO audit: ugi=lifeng	ip=unknown-ip-addr	cmd=get_all_databases	
17/03/28 12:31:12 INFO HiveMetaStore: 0: get_functions: db=default pat=*
17/03/28 12:31:12 INFO audit: ugi=lifeng	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
17/03/28 12:31:12 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 12:31:14 INFO SessionState: Created local directory: C:/Users/lifeng/AppData/Local/Temp/fa0a0413-9de5-49bc-98f3-e0ac49b61c3a_resources
17/03/28 12:31:14 INFO SessionState: Created HDFS directory: D:/Spark/spark-1.6.2-bin-hadoop2.6/tmp/hive/lifeng/fa0a0413-9de5-49bc-98f3-e0ac49b61c3a
17/03/28 12:31:14 INFO SessionState: Created local directory: D:/Spark/spark-1.6.2-bin-hadoop2.6/tmp/hive/fa0a0413-9de5-49bc-98f3-e0ac49b61c3a
17/03/28 12:31:14 INFO SessionState: Created HDFS directory: D:/Spark/spark-1.6.2-bin-hadoop2.6/tmp/hive/lifeng/fa0a0413-9de5-49bc-98f3-e0ac49b61c3a/_tmp_space.db
17/03/28 12:31:14 INFO HiveContext: default warehouse location is D:\Spark\spark-1.6.2-bin-hadoop2.6\tmp\hive
17/03/28 12:31:14 INFO HiveContext: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
17/03/28 12:31:14 INFO ClientWrapper: Inspected Hadoop version: 2.6.0
17/03/28 12:31:14 INFO ClientWrapper: Loaded org.apache.hadoop.hive.shims.Hadoop23Shims for Hadoop version 2.6.0
17/03/28 12:31:14 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/03/28 12:31:14 INFO ObjectStore: ObjectStore, initialize called
17/03/28 12:31:14 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
17/03/28 12:31:14 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
17/03/28 12:31:14 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
17/03/28 12:31:15 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
17/03/28 12:31:15 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
17/03/28 12:31:16 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 12:31:16 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 12:31:17 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 12:31:17 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 12:31:17 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
17/03/28 12:31:17 INFO ObjectStore: Initialized ObjectStore
17/03/28 12:31:17 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
17/03/28 12:31:17 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
17/03/28 12:31:17 INFO HiveMetaStore: Added admin role in metastore
17/03/28 12:31:17 INFO HiveMetaStore: Added public role in metastore
17/03/28 12:31:17 INFO HiveMetaStore: No user is added in admin role, since config is empty
17/03/28 12:31:17 INFO HiveMetaStore: 0: get_all_databases
17/03/28 12:31:17 INFO audit: ugi=lifeng	ip=unknown-ip-addr	cmd=get_all_databases	
17/03/28 12:31:17 INFO HiveMetaStore: 0: get_functions: db=default pat=*
17/03/28 12:31:17 INFO audit: ugi=lifeng	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
17/03/28 12:31:17 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
17/03/28 12:31:17 INFO SessionState: Created local directory: C:/Users/lifeng/AppData/Local/Temp/e41b7a65-bcce-4779-8254-b83150d9e840_resources
17/03/28 12:31:17 INFO SessionState: Created HDFS directory: D:/Spark/spark-1.6.2-bin-hadoop2.6/tmp/hive/lifeng/e41b7a65-bcce-4779-8254-b83150d9e840
17/03/28 12:31:18 INFO SessionState: Created local directory: D:/Spark/spark-1.6.2-bin-hadoop2.6/tmp/hive/e41b7a65-bcce-4779-8254-b83150d9e840
17/03/28 12:31:18 INFO SessionState: Created HDFS directory: D:/Spark/spark-1.6.2-bin-hadoop2.6/tmp/hive/lifeng/e41b7a65-bcce-4779-8254-b83150d9e840/_tmp_space.db
17/03/28 12:31:40 INFO HiveMetaStore: 0: get_tables: db=default pat=.*
17/03/28 12:31:40 INFO audit: ugi=lifeng	ip=unknown-ip-addr	cmd=get_tables: db=default pat=.*	
17/03/28 12:31:40 INFO SparkContext: Starting job: collect at utils.scala:59
17/03/28 12:31:40 INFO DAGScheduler: Got job 0 (collect at utils.scala:59) with 1 output partitions
17/03/28 12:31:40 INFO DAGScheduler: Final stage: ResultStage 0 (collect at utils.scala:59)
17/03/28 12:31:40 INFO DAGScheduler: Parents of final stage: List()
17/03/28 12:31:40 INFO DAGScheduler: Missing parents: List()
17/03/28 12:31:40 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at map at utils.scala:56), which has no missing parents
17/03/28 12:31:40 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 5.4 KB, free 5.4 KB)
17/03/28 12:31:40 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.0 KB, free 8.4 KB)
17/03/28 12:31:40 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:51431 (size: 3.0 KB, free: 511.1 MB)
17/03/28 12:31:40 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1006
17/03/28 12:31:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at map at utils.scala:56)
17/03/28 12:31:40 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
17/03/28 12:31:40 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2348 bytes)
17/03/28 12:31:40 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
17/03/28 12:31:40 INFO Executor: Fetching http://127.0.0.1:51414/jars/sparklyr-1.6-2.10.jar with timestamp 1490675445927
17/03/28 12:31:40 INFO Utils: Fetching http://127.0.0.1:51414/jars/sparklyr-1.6-2.10.jar to C:\Users\lifeng\AppData\Local\Temp\spark-30a2418b-c83a-4d4d-86af-0c5d2397bf7f\userFiles-b7d8cea0-c626-47d9-85b7-e90de11d84b3\fetchFileTemp5439026264603005403.tmp
17/03/28 12:31:40 INFO Executor: Adding file:/C:/Users/lifeng/AppData/Local/Temp/spark-30a2418b-c83a-4d4d-86af-0c5d2397bf7f/userFiles-b7d8cea0-c626-47d9-85b7-e90de11d84b3/sparklyr-1.6-2.10.jar to class loader
17/03/28 12:31:40 INFO Executor: Fetching http://127.0.0.1:51414/jars/univocity-parsers-1.5.1.jar with timestamp 1490675445926
17/03/28 12:31:40 INFO Utils: Fetching http://127.0.0.1:51414/jars/univocity-parsers-1.5.1.jar to C:\Users\lifeng\AppData\Local\Temp\spark-30a2418b-c83a-4d4d-86af-0c5d2397bf7f\userFiles-b7d8cea0-c626-47d9-85b7-e90de11d84b3\fetchFileTemp6104230228406735772.tmp
17/03/28 12:31:40 INFO Executor: Adding file:/C:/Users/lifeng/AppData/Local/Temp/spark-30a2418b-c83a-4d4d-86af-0c5d2397bf7f/userFiles-b7d8cea0-c626-47d9-85b7-e90de11d84b3/univocity-parsers-1.5.1.jar to class loader
17/03/28 12:31:40 INFO Executor: Fetching http://127.0.0.1:51414/jars/spark-csv_2.11-1.3.0.jar with timestamp 1490675445922
17/03/28 12:31:40 INFO Utils: Fetching http://127.0.0.1:51414/jars/spark-csv_2.11-1.3.0.jar to C:\Users\lifeng\AppData\Local\Temp\spark-30a2418b-c83a-4d4d-86af-0c5d2397bf7f\userFiles-b7d8cea0-c626-47d9-85b7-e90de11d84b3\fetchFileTemp451549861572662602.tmp
17/03/28 12:31:40 INFO Executor: Adding file:/C:/Users/lifeng/AppData/Local/Temp/spark-30a2418b-c83a-4d4d-86af-0c5d2397bf7f/userFiles-b7d8cea0-c626-47d9-85b7-e90de11d84b3/spark-csv_2.11-1.3.0.jar to class loader
17/03/28 12:31:40 INFO Executor: Fetching http://127.0.0.1:51414/jars/commons-csv-1.1.jar with timestamp 1490675445924
17/03/28 12:31:40 INFO Utils: Fetching http://127.0.0.1:51414/jars/commons-csv-1.1.jar to C:\Users\lifeng\AppData\Local\Temp\spark-30a2418b-c83a-4d4d-86af-0c5d2397bf7f\userFiles-b7d8cea0-c626-47d9-85b7-e90de11d84b3\fetchFileTemp1472246525404289133.tmp
17/03/28 12:31:41 INFO Executor: Adding file:/C:/Users/lifeng/AppData/Local/Temp/spark-30a2418b-c83a-4d4d-86af-0c5d2397bf7f/userFiles-b7d8cea0-c626-47d9-85b7-e90de11d84b3/commons-csv-1.1.jar to class loader
17/03/28 12:31:41 INFO GenerateUnsafeProjection: Code generated in 120.6403 ms
17/03/28 12:31:41 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1060 bytes result sent to driver
17/03/28 12:31:41 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 670 ms on localhost (1/1)
17/03/28 12:31:41 INFO DAGScheduler: ResultStage 0 (collect at utils.scala:59) finished in 0.683 s
17/03/28 12:31:41 INFO DAGScheduler: Job 0 finished: collect at utils.scala:59, took 0.796926 s
17/03/28 12:31:41 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
17/03/28 12:31:41 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 61.8 KB, free 70.2 KB)
17/03/28 12:31:41 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 19.3 KB, free 89.5 KB)
17/03/28 12:31:41 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:51431 (size: 19.3 KB, free: 511.1 MB)
17/03/28 12:31:41 INFO SparkContext: Created broadcast 1 from textFile at TextFile.scala:30
17/03/28 12:31:41 INFO FileInputFormat: Total input paths to process : 1
17/03/28 12:31:41 INFO SparkContext: Starting job: take at CsvRelation.scala:249
17/03/28 12:31:41 INFO DAGScheduler: Got job 1 (take at CsvRelation.scala:249) with 1 output partitions
17/03/28 12:31:41 INFO DAGScheduler: Final stage: ResultStage 1 (take at CsvRelation.scala:249)
17/03/28 12:31:41 INFO DAGScheduler: Parents of final stage: List()
17/03/28 12:31:41 INFO DAGScheduler: Missing parents: List()
17/03/28 12:31:41 INFO DAGScheduler: Submitting ResultStage 1 (C:\Users\lifeng\AppData\Local\Temp\RtmpobpWe6/spark_serialize_9fd0bf1861994b0d294634211269ec9e591b014b83a5683f179dd18e7e70ef0b.csv MapPartitionsRDD[5] at textFile at TextFile.scala:30), which has no missing parents
17/03/28 12:31:41 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 3.2 KB, free 92.7 KB)
17/03/28 12:31:41 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 1946.0 B, free 94.6 KB)
17/03/28 12:31:41 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:51431 (size: 1946.0 B, free: 511.1 MB)
17/03/28 12:31:41 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1006
17/03/28 12:31:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (C:\Users\lifeng\AppData\Local\Temp\RtmpobpWe6/spark_serialize_9fd0bf1861994b0d294634211269ec9e591b014b83a5683f179dd18e7e70ef0b.csv MapPartitionsRDD[5] at textFile at TextFile.scala:30)
17/03/28 12:31:41 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
17/03/28 12:31:41 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2482 bytes)
17/03/28 12:31:41 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
17/03/28 12:31:41 INFO HadoopRDD: Input split: file:/C:/Users/lifeng/AppData/Local/Temp/RtmpobpWe6/spark_serialize_9fd0bf1861994b0d294634211269ec9e591b014b83a5683f179dd18e7e70ef0b.csv:0+2088
17/03/28 12:31:41 INFO deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id
17/03/28 12:31:41 INFO deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
17/03/28 12:31:41 INFO deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
17/03/28 12:31:41 INFO deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
17/03/28 12:31:41 INFO deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id
17/03/28 12:31:41 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2354 bytes result sent to driver
17/03/28 12:31:41 INFO DAGScheduler: ResultStage 1 (take at CsvRelation.scala:249) finished in 0.051 s
17/03/28 12:31:41 INFO DAGScheduler: Job 1 finished: take at CsvRelation.scala:249, took 0.061802 s
17/03/28 12:31:41 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 51 ms on localhost (1/1)
17/03/28 12:31:41 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
17/03/28 12:31:41 INFO ParseDriver: Parsing command: SELECT * FROM  `iris`
17/03/28 12:31:42 INFO ParseDriver: Parse Completed
17/03/28 12:31:42 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 208.5 KB, free 303.1 KB)
17/03/28 12:31:42 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 19.3 KB, free 322.5 KB)
17/03/28 12:31:42 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on localhost:51431 (size: 19.3 KB, free: 511.1 MB)
17/03/28 12:31:42 INFO SparkContext: Created broadcast 3 from textFile at TextFile.scala:30
17/03/28 12:31:42 INFO FileInputFormat: Total input paths to process : 1
17/03/28 12:31:42 INFO SparkContext: Starting job: take at CsvRelation.scala:249
17/03/28 12:31:42 INFO DAGScheduler: Got job 2 (take at CsvRelation.scala:249) with 1 output partitions
17/03/28 12:31:42 INFO DAGScheduler: Final stage: ResultStage 2 (take at CsvRelation.scala:249)
17/03/28 12:31:42 INFO DAGScheduler: Parents of final stage: List()
17/03/28 12:31:42 INFO DAGScheduler: Missing parents: List()
17/03/28 12:31:42 INFO DAGScheduler: Submitting ResultStage 2 (C:\Users\lifeng\AppData\Local\Temp\RtmpobpWe6/spark_serialize_9fd0bf1861994b0d294634211269ec9e591b014b83a5683f179dd18e7e70ef0b.csv MapPartitionsRDD[7] at textFile at TextFile.scala:30), which has no missing parents
17/03/28 12:31:42 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 3.2 KB, free 325.7 KB)
17/03/28 12:31:42 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 1947.0 B, free 327.6 KB)
17/03/28 12:31:42 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on localhost:51431 (size: 1947.0 B, free: 511.1 MB)
17/03/28 12:31:42 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1006
17/03/28 12:31:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (C:\Users\lifeng\AppData\Local\Temp\RtmpobpWe6/spark_serialize_9fd0bf1861994b0d294634211269ec9e591b014b83a5683f179dd18e7e70ef0b.csv MapPartitionsRDD[7] at textFile at TextFile.scala:30)
17/03/28 12:31:42 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
17/03/28 12:31:42 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0,PROCESS_LOCAL, 2482 bytes)
17/03/28 12:31:42 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
17/03/28 12:31:42 INFO HadoopRDD: Input split: file:/C:/Users/lifeng/AppData/Local/Temp/RtmpobpWe6/spark_serialize_9fd0bf1861994b0d294634211269ec9e591b014b83a5683f179dd18e7e70ef0b.csv:0+2088
17/03/28 12:31:42 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2354 bytes result sent to driver
17/03/28 12:31:42 INFO DAGScheduler: ResultStage 2 (take at CsvRelation.scala:249) finished in 0.009 s
17/03/28 12:31:42 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 9 ms on localhost (1/1)
17/03/28 12:31:42 INFO DAGScheduler: Job 2 finished: take at CsvRelation.scala:249, took 0.018400 s
17/03/28 12:31:42 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
17/03/28 12:31:42 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 208.5 KB, free 536.1 KB)
17/03/28 12:31:42 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 19.3 KB, free 555.4 KB)
17/03/28 12:31:42 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on localhost:51431 (size: 19.3 KB, free: 511.1 MB)
17/03/28 12:31:42 INFO SparkContext: Created broadcast 5 from textFile at TextFile.scala:30
17/03/28 12:31:42 INFO FileInputFormat: Total input paths to process : 1
17/03/28 12:31:42 INFO SparkContext: Starting job: sql at NativeMethodAccessorImpl.java:-2
17/03/28 12:31:42 INFO DAGScheduler: Registering RDD 17 (sql at NativeMethodAccessorImpl.java:-2)
17/03/28 12:31:42 INFO DAGScheduler: Got job 3 (sql at NativeMethodAccessorImpl.java:-2) with 1 output partitions
17/03/28 12:31:42 INFO DAGScheduler: Final stage: ResultStage 4 (sql at NativeMethodAccessorImpl.java:-2)
17/03/28 12:31:42 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
17/03/28 12:31:42 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 3)
17/03/28 12:31:42 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[17] at sql at NativeMethodAccessorImpl.java:-2), which has no missing parents
17/03/28 12:31:42 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 18.3 KB, free 573.7 KB)
17/03/28 12:31:42 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 8.7 KB, free 582.3 KB)
17/03/28 12:31:42 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on localhost:51431 (size: 8.7 KB, free: 511.1 MB)
17/03/28 12:31:42 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1006
17/03/28 12:31:42 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[17] at sql at NativeMethodAccessorImpl.java:-2)
17/03/28 12:31:42 INFO TaskSchedulerImpl: Adding task set 3.0 with 2 tasks
17/03/28 12:31:42 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, localhost, partition 0,PROCESS_LOCAL, 2471 bytes)
17/03/28 12:31:42 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 4, localhost, partition 1,PROCESS_LOCAL, 2471 bytes)
17/03/28 12:31:42 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
17/03/28 12:31:42 INFO Executor: Running task 1.0 in stage 3.0 (TID 4)
17/03/28 12:31:42 INFO CacheManager: Partition rdd_14_1 not found, computing it
17/03/28 12:31:42 INFO HadoopRDD: Input split: file:/C:/Users/lifeng/AppData/Local/Temp/RtmpobpWe6/spark_serialize_9fd0bf1861994b0d294634211269ec9e591b014b83a5683f179dd18e7e70ef0b.csv:2088+2089
17/03/28 12:31:42 INFO CacheManager: Partition rdd_14_0 not found, computing it
17/03/28 12:31:42 INFO HadoopRDD: Input split: file:/C:/Users/lifeng/AppData/Local/Temp/RtmpobpWe6/spark_serialize_9fd0bf1861994b0d294634211269ec9e591b014b83a5683f179dd18e7e70ef0b.csv:0+2088
17/03/28 12:31:42 INFO GenerateUnsafeProjection: Code generated in 17.783295 ms
17/03/28 12:31:42 INFO MemoryStore: Block rdd_14_0 stored as values in memory (estimated size 3.2 KB, free 585.5 KB)
17/03/28 12:31:42 INFO MemoryStore: Block rdd_14_1 stored as values in memory (estimated size 3.1 KB, free 588.7 KB)
17/03/28 12:31:42 INFO BlockManagerInfo: Added rdd_14_0 in memory on localhost:51431 (size: 3.2 KB, free: 511.1 MB)
17/03/28 12:31:42 INFO BlockManagerInfo: Added rdd_14_1 in memory on localhost:51431 (size: 3.1 KB, free: 511.0 MB)
17/03/28 12:31:42 INFO GeneratePredicate: Code generated in 6.784785 ms
17/03/28 12:31:42 INFO GenerateColumnAccessor: Code generated in 19.230405 ms
17/03/28 12:31:42 INFO GenerateMutableProjection: Code generated in 6.024292 ms
17/03/28 12:31:42 INFO GenerateUnsafeProjection: Code generated in 6.270415 ms
17/03/28 12:31:42 INFO GenerateMutableProjection: Code generated in 9.890758 ms
17/03/28 12:31:42 INFO GenerateUnsafeRowJoiner: Code generated in 5.738662 ms
17/03/28 12:31:42 INFO GenerateUnsafeProjection: Code generated in 7.005624 ms
17/03/28 12:31:42 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 3784 bytes result sent to driver
17/03/28 12:31:42 INFO Executor: Finished task 1.0 in stage 3.0 (TID 4). 3787 bytes result sent to driver
17/03/28 12:31:42 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 4) in 292 ms on localhost (1/2)
17/03/28 12:31:42 INFO DAGScheduler: ShuffleMapStage 3 (sql at NativeMethodAccessorImpl.java:-2) finished in 0.296 s
17/03/28 12:31:42 INFO DAGScheduler: looking for newly runnable stages
17/03/28 12:31:42 INFO DAGScheduler: running: Set()
17/03/28 12:31:42 INFO DAGScheduler: waiting: Set(ResultStage 4)
17/03/28 12:31:42 INFO DAGScheduler: failed: Set()
17/03/28 12:31:42 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[20] at sql at NativeMethodAccessorImpl.java:-2), which has no missing parents
17/03/28 12:31:42 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 296 ms on localhost (2/2)
17/03/28 12:31:42 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
17/03/28 12:31:42 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 9.3 KB, free 598.0 KB)
17/03/28 12:31:42 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 4.6 KB, free 602.6 KB)
17/03/28 12:31:42 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:51431 (size: 4.6 KB, free: 511.0 MB)
17/03/28 12:31:42 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1006
17/03/28 12:31:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[20] at sql at NativeMethodAccessorImpl.java:-2)
17/03/28 12:31:42 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks
17/03/28 12:31:42 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 5, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/03/28 12:31:42 INFO Executor: Running task 0.0 in stage 4.0 (TID 5)
17/03/28 12:31:42 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/03/28 12:31:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
17/03/28 12:31:42 INFO GenerateMutableProjection: Code generated in 6.132538 ms
17/03/28 12:31:42 INFO GenerateMutableProjection: Code generated in 5.762366 ms
17/03/28 12:31:42 INFO Executor: Finished task 0.0 in stage 4.0 (TID 5). 1830 bytes result sent to driver
17/03/28 12:31:42 INFO DAGScheduler: ResultStage 4 (sql at NativeMethodAccessorImpl.java:-2) finished in 0.080 s
17/03/28 12:31:42 INFO DAGScheduler: Job 3 finished: sql at NativeMethodAccessorImpl.java:-2, took 0.415535 s
17/03/28 12:31:42 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 5) in 80 ms on localhost (1/1)
17/03/28 12:31:42 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
17/03/28 12:31:42 INFO ParseDriver: Parsing command: SELECT count(*) FROM  `iris`
17/03/28 12:31:42 INFO ParseDriver: Parse Completed
17/03/28 12:31:43 INFO SparkContext: Starting job: collect at utils.scala:195
17/03/28 12:31:43 INFO DAGScheduler: Registering RDD 24 (collect at utils.scala:195)
17/03/28 12:31:43 INFO DAGScheduler: Got job 4 (collect at utils.scala:195) with 1 output partitions
17/03/28 12:31:43 INFO DAGScheduler: Final stage: ResultStage 6 (collect at utils.scala:195)
17/03/28 12:31:43 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)
17/03/28 12:31:43 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 5)
17/03/28 12:31:43 INFO DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[24] at collect at utils.scala:195), which has no missing parents
17/03/28 12:31:43 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 18.4 KB, free 620.9 KB)
17/03/28 12:31:43 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 8.7 KB, free 629.6 KB)
17/03/28 12:31:43 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on localhost:51431 (size: 8.7 KB, free: 511.0 MB)
17/03/28 12:31:43 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1006
17/03/28 12:31:43 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[24] at collect at utils.scala:195)
17/03/28 12:31:43 INFO TaskSchedulerImpl: Adding task set 5.0 with 2 tasks
17/03/28 12:31:43 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 6, localhost, partition 0,PROCESS_LOCAL, 2471 bytes)
17/03/28 12:31:43 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 7, localhost, partition 1,PROCESS_LOCAL, 2471 bytes)
17/03/28 12:31:43 INFO Executor: Running task 0.0 in stage 5.0 (TID 6)
17/03/28 12:31:43 INFO Executor: Running task 1.0 in stage 5.0 (TID 7)
17/03/28 12:31:43 INFO BlockManager: Found block rdd_14_1 locally
17/03/28 12:31:43 INFO BlockManager: Found block rdd_14_0 locally
17/03/28 12:31:43 INFO Executor: Finished task 1.0 in stage 5.0 (TID 7). 2679 bytes result sent to driver
17/03/28 12:31:43 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 7) in 22 ms on localhost (1/2)
17/03/28 12:31:43 INFO Executor: Finished task 0.0 in stage 5.0 (TID 6). 2679 bytes result sent to driver
17/03/28 12:31:43 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 6) in 25 ms on localhost (2/2)
17/03/28 12:31:43 INFO DAGScheduler: ShuffleMapStage 5 (collect at utils.scala:195) finished in 0.025 s
17/03/28 12:31:43 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
17/03/28 12:31:43 INFO DAGScheduler: looking for newly runnable stages
17/03/28 12:31:43 INFO DAGScheduler: running: Set()
17/03/28 12:31:43 INFO DAGScheduler: waiting: Set(ResultStage 6)
17/03/28 12:31:43 INFO DAGScheduler: failed: Set()
17/03/28 12:31:43 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[27] at collect at utils.scala:195), which has no missing parents
17/03/28 12:31:43 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 9.4 KB, free 639.0 KB)
17/03/28 12:31:43 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 4.6 KB, free 643.6 KB)
17/03/28 12:31:43 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on localhost:51431 (size: 4.6 KB, free: 511.0 MB)
17/03/28 12:31:43 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1006
17/03/28 12:31:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[27] at collect at utils.scala:195)
17/03/28 12:31:43 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks
17/03/28 12:31:43 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 8, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/03/28 12:31:43 INFO Executor: Running task 0.0 in stage 6.0 (TID 8)
17/03/28 12:31:43 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/03/28 12:31:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/03/28 12:31:43 INFO Executor: Finished task 0.0 in stage 6.0 (TID 8). 1830 bytes result sent to driver
17/03/28 12:31:43 INFO DAGScheduler: ResultStage 6 (collect at utils.scala:195) finished in 0.009 s
17/03/28 12:31:43 INFO DAGScheduler: Job 4 finished: collect at utils.scala:195, took 0.051871 s
17/03/28 12:31:43 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 8) in 8 ms on localhost (1/1)
17/03/28 12:31:43 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
17/03/28 12:31:43 INFO ParseDriver: Parsing command: SELECT *
FROM `iris` AS `zzz1`
WHERE (0 = 1)
17/03/28 12:31:43 INFO ParseDriver: Parse Completed
17/03/28 12:31:43 INFO HiveMetaStore: 0: get_tables: db=default pat=.*
17/03/28 12:31:43 INFO audit: ugi=lifeng	ip=unknown-ip-addr	cmd=get_tables: db=default pat=.*	
17/03/28 12:31:43 INFO ContextCleaner: Cleaned accumulator 9
17/03/28 12:31:43 INFO ContextCleaner: Cleaned accumulator 10
17/03/28 12:31:43 INFO ContextCleaner: Cleaned accumulator 8
17/03/28 12:31:43 INFO ContextCleaner: Cleaned accumulator 7
17/03/28 12:31:43 INFO SparkContext: Starting job: collect at utils.scala:195
17/03/28 12:31:43 INFO DAGScheduler: Got job 5 (collect at utils.scala:195) with 1 output partitions
17/03/28 12:31:43 INFO DAGScheduler: Final stage: ResultStage 7 (collect at utils.scala:195)
17/03/28 12:31:43 INFO DAGScheduler: Parents of final stage: List()
17/03/28 12:31:43 INFO DAGScheduler: Missing parents: List()
17/03/28 12:31:43 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[29] at collect at utils.scala:195), which has no missing parents
17/03/28 12:31:43 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 1968.0 B, free 645.5 KB)
17/03/28 12:31:43 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 1224.0 B, free 646.7 KB)
17/03/28 12:31:43 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on localhost:51431 (size: 1224.0 B, free: 511.0 MB)
17/03/28 12:31:43 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1006
17/03/28 12:31:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[29] at collect at utils.scala:195)
17/03/28 12:31:43 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks
17/03/28 12:31:43 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 9, localhost, partition 0,PROCESS_LOCAL, 2646 bytes)
17/03/28 12:31:43 INFO Executor: Running task 0.0 in stage 7.0 (TID 9)
17/03/28 12:31:43 INFO Executor: Finished task 0.0 in stage 7.0 (TID 9). 1258 bytes result sent to driver
17/03/28 12:31:43 INFO DAGScheduler: ResultStage 7 (collect at utils.scala:195) finished in 0.008 s
17/03/28 12:31:43 INFO DAGScheduler: Job 5 finished: collect at utils.scala:195, took 0.015120 s
17/03/28 12:31:43 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 9) in 8 ms on localhost (1/1)
17/03/28 12:31:43 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
17/03/28 12:31:43 INFO BlockManagerInfo: Removed broadcast_4_piece0 on localhost:51431 in memory (size: 1947.0 B, free: 511.0 MB)
17/03/28 12:31:43 INFO ContextCleaner: Cleaned accumulator 5
17/03/28 12:31:43 INFO BlockManagerInfo: Removed broadcast_3_piece0 on localhost:51431 in memory (size: 19.3 KB, free: 511.0 MB)
17/03/28 12:31:43 INFO BlockManagerInfo: Removed broadcast_2_piece0 on localhost:51431 in memory (size: 1946.0 B, free: 511.1 MB)
17/03/28 12:31:43 INFO ContextCleaner: Cleaned accumulator 4
17/03/28 12:31:43 INFO BlockManagerInfo: Removed broadcast_1_piece0 on localhost:51431 in memory (size: 19.3 KB, free: 511.1 MB)
17/03/28 12:31:43 INFO BlockManagerInfo: Removed broadcast_0_piece0 on localhost:51431 in memory (size: 3.0 KB, free: 511.1 MB)
17/03/28 12:31:43 INFO ContextCleaner: Cleaned accumulator 3
17/03/28 12:31:43 INFO ContextCleaner: Cleaned accumulator 2
17/03/28 12:31:43 INFO BlockManagerInfo: Removed broadcast_9_piece0 on localhost:51431 in memory (size: 4.6 KB, free: 511.1 MB)
17/03/28 12:31:43 INFO ContextCleaner: Cleaned accumulator 26
17/03/28 12:31:43 INFO BlockManagerInfo: Removed broadcast_8_piece0 on localhost:51431 in memory (size: 8.7 KB, free: 511.1 MB)
17/03/28 12:31:43 INFO ContextCleaner: Cleaned accumulator 25
17/03/28 12:31:43 INFO ContextCleaner: Cleaned shuffle 1
17/03/28 12:31:43 INFO BlockManagerInfo: Removed broadcast_7_piece0 on localhost:51431 in memory (size: 4.6 KB, free: 511.1 MB)
17/03/28 12:31:43 INFO ContextCleaner: Cleaned accumulator 16
17/03/28 12:31:43 INFO BlockManagerInfo: Removed broadcast_6_piece0 on localhost:51431 in memory (size: 8.7 KB, free: 511.1 MB)
17/03/28 12:31:43 INFO ContextCleaner: Cleaned accumulator 15
17/03/28 12:31:43 INFO ContextCleaner: Cleaned shuffle 0
17/03/28 12:31:43 INFO ContextCleaner: Cleaned accumulator 14
17/03/28 12:31:43 INFO ContextCleaner: Cleaned accumulator 13
17/03/28 12:31:43 INFO ContextCleaner: Cleaned accumulator 12
17/03/28 12:31:43 INFO ContextCleaner: Cleaned accumulator 11
17/03/28 12:32:07 INFO HiveMetaStore: 0: get_tables: db=default pat=.*
17/03/28 12:32:07 INFO audit: ugi=lifeng	ip=unknown-ip-addr	cmd=get_tables: db=default pat=.*	
17/03/28 12:32:07 INFO SparkContext: Starting job: collect at utils.scala:59
17/03/28 12:32:07 INFO DAGScheduler: Got job 6 (collect at utils.scala:59) with 1 output partitions
17/03/28 12:32:07 INFO DAGScheduler: Final stage: ResultStage 8 (collect at utils.scala:59)
17/03/28 12:32:07 INFO DAGScheduler: Parents of final stage: List()
17/03/28 12:32:07 INFO DAGScheduler: Missing parents: List()
17/03/28 12:32:07 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[33] at map at utils.scala:56), which has no missing parents
17/03/28 12:32:07 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 5.4 KB, free 242.7 KB)
17/03/28 12:32:07 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 3.0 KB, free 245.7 KB)
17/03/28 12:32:07 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on localhost:51431 (size: 3.0 KB, free: 511.1 MB)
17/03/28 12:32:07 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1006
17/03/28 12:32:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[33] at map at utils.scala:56)
17/03/28 12:32:07 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks
17/03/28 12:32:07 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 10, localhost, partition 0,PROCESS_LOCAL, 2646 bytes)
17/03/28 12:32:07 INFO Executor: Running task 0.0 in stage 8.0 (TID 10)
17/03/28 12:32:07 INFO Executor: Finished task 0.0 in stage 8.0 (TID 10). 1067 bytes result sent to driver
17/03/28 12:32:07 INFO DAGScheduler: ResultStage 8 (collect at utils.scala:59) finished in 0.003 s
17/03/28 12:32:07 INFO DAGScheduler: Job 6 finished: collect at utils.scala:59, took 0.009073 s
17/03/28 12:32:07 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 10) in 3 ms on localhost (1/1)
17/03/28 12:32:07 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
17/03/28 12:32:16 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 208.5 KB, free 454.2 KB)
17/03/28 12:32:16 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 19.3 KB, free 473.5 KB)
17/03/28 12:32:16 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on localhost:51431 (size: 19.3 KB, free: 511.1 MB)
17/03/28 12:32:16 INFO SparkContext: Created broadcast 12 from textFile at TextFile.scala:30
17/03/28 12:32:16 INFO FileInputFormat: Total input paths to process : 1
17/03/28 12:32:16 INFO SparkContext: Starting job: take at CsvRelation.scala:249
17/03/28 12:32:16 INFO DAGScheduler: Got job 7 (take at CsvRelation.scala:249) with 1 output partitions
17/03/28 12:32:16 INFO DAGScheduler: Final stage: ResultStage 9 (take at CsvRelation.scala:249)
17/03/28 12:32:16 INFO DAGScheduler: Parents of final stage: List()
17/03/28 12:32:16 INFO DAGScheduler: Missing parents: List()
17/03/28 12:32:16 INFO DAGScheduler: Submitting ResultStage 9 (C:\Users\lifeng\AppData\Local\Temp\RtmpobpWe6/spark_serialize_bdf64b31db10334a157167e3720fb5fda677e92a85e2e167238fb89c721869ff.csv MapPartitionsRDD[35] at textFile at TextFile.scala:30), which has no missing parents
17/03/28 12:32:16 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 3.2 KB, free 476.7 KB)
17/03/28 12:32:16 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 1945.0 B, free 478.6 KB)
17/03/28 12:32:16 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on localhost:51431 (size: 1945.0 B, free: 511.1 MB)
17/03/28 12:32:16 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1006
17/03/28 12:32:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (C:\Users\lifeng\AppData\Local\Temp\RtmpobpWe6/spark_serialize_bdf64b31db10334a157167e3720fb5fda677e92a85e2e167238fb89c721869ff.csv MapPartitionsRDD[35] at textFile at TextFile.scala:30)
17/03/28 12:32:16 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks
17/03/28 12:32:16 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 11, localhost, partition 0,PROCESS_LOCAL, 2482 bytes)
17/03/28 12:32:16 INFO Executor: Running task 0.0 in stage 9.0 (TID 11)
17/03/28 12:32:16 INFO HadoopRDD: Input split: file:/C:/Users/lifeng/AppData/Local/Temp/RtmpobpWe6/spark_serialize_bdf64b31db10334a157167e3720fb5fda677e92a85e2e167238fb89c721869ff.csv:0+16824941
17/03/28 12:32:16 INFO Executor: Finished task 0.0 in stage 9.0 (TID 11). 3117 bytes result sent to driver
17/03/28 12:32:16 INFO DAGScheduler: ResultStage 9 (take at CsvRelation.scala:249) finished in 0.005 s
17/03/28 12:32:16 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 11) in 5 ms on localhost (1/1)
17/03/28 12:32:16 INFO DAGScheduler: Job 7 finished: take at CsvRelation.scala:249, took 0.010258 s
17/03/28 12:32:16 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
17/03/28 12:32:16 INFO ParseDriver: Parsing command: SELECT * FROM  `flights`
17/03/28 12:32:16 INFO ParseDriver: Parse Completed
17/03/28 12:32:16 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 208.5 KB, free 687.1 KB)
17/03/28 12:32:16 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 19.3 KB, free 706.5 KB)
17/03/28 12:32:16 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on localhost:51431 (size: 19.3 KB, free: 511.1 MB)
17/03/28 12:32:16 INFO SparkContext: Created broadcast 14 from textFile at TextFile.scala:30
17/03/28 12:32:16 INFO FileInputFormat: Total input paths to process : 1
17/03/28 12:32:16 INFO SparkContext: Starting job: take at CsvRelation.scala:249
17/03/28 12:32:16 INFO DAGScheduler: Got job 8 (take at CsvRelation.scala:249) with 1 output partitions
17/03/28 12:32:16 INFO DAGScheduler: Final stage: ResultStage 10 (take at CsvRelation.scala:249)
17/03/28 12:32:16 INFO DAGScheduler: Parents of final stage: List()
17/03/28 12:32:16 INFO DAGScheduler: Missing parents: List()
17/03/28 12:32:16 INFO DAGScheduler: Submitting ResultStage 10 (C:\Users\lifeng\AppData\Local\Temp\RtmpobpWe6/spark_serialize_bdf64b31db10334a157167e3720fb5fda677e92a85e2e167238fb89c721869ff.csv MapPartitionsRDD[37] at textFile at TextFile.scala:30), which has no missing parents
17/03/28 12:32:16 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 3.2 KB, free 709.7 KB)
17/03/28 12:32:16 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 1944.0 B, free 711.6 KB)
17/03/28 12:32:16 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on localhost:51431 (size: 1944.0 B, free: 511.1 MB)
17/03/28 12:32:16 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1006
17/03/28 12:32:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (C:\Users\lifeng\AppData\Local\Temp\RtmpobpWe6/spark_serialize_bdf64b31db10334a157167e3720fb5fda677e92a85e2e167238fb89c721869ff.csv MapPartitionsRDD[37] at textFile at TextFile.scala:30)
17/03/28 12:32:16 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks
17/03/28 12:32:16 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 12, localhost, partition 0,PROCESS_LOCAL, 2482 bytes)
17/03/28 12:32:16 INFO Executor: Running task 0.0 in stage 10.0 (TID 12)
17/03/28 12:32:16 INFO HadoopRDD: Input split: file:/C:/Users/lifeng/AppData/Local/Temp/RtmpobpWe6/spark_serialize_bdf64b31db10334a157167e3720fb5fda677e92a85e2e167238fb89c721869ff.csv:0+16824941
17/03/28 12:32:16 INFO Executor: Finished task 0.0 in stage 10.0 (TID 12). 3117 bytes result sent to driver
17/03/28 12:32:16 INFO DAGScheduler: ResultStage 10 (take at CsvRelation.scala:249) finished in 0.008 s
17/03/28 12:32:16 INFO DAGScheduler: Job 8 finished: take at CsvRelation.scala:249, took 0.013736 s
17/03/28 12:32:16 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 12) in 7 ms on localhost (1/1)
17/03/28 12:32:16 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
17/03/28 12:32:16 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 208.5 KB, free 920.1 KB)
17/03/28 12:32:16 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 19.3 KB, free 939.4 KB)
17/03/28 12:32:16 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on localhost:51431 (size: 19.3 KB, free: 511.0 MB)
17/03/28 12:32:16 INFO SparkContext: Created broadcast 16 from textFile at TextFile.scala:30
17/03/28 12:32:16 INFO FileInputFormat: Total input paths to process : 1
17/03/28 12:32:16 INFO SparkContext: Starting job: sql at NativeMethodAccessorImpl.java:-2
17/03/28 12:32:16 INFO DAGScheduler: Registering RDD 47 (sql at NativeMethodAccessorImpl.java:-2)
17/03/28 12:32:16 INFO DAGScheduler: Got job 9 (sql at NativeMethodAccessorImpl.java:-2) with 1 output partitions
17/03/28 12:32:16 INFO DAGScheduler: Final stage: ResultStage 12 (sql at NativeMethodAccessorImpl.java:-2)
17/03/28 12:32:16 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 11)
17/03/28 12:32:16 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 11)
17/03/28 12:32:16 INFO DAGScheduler: Submitting ShuffleMapStage 11 (MapPartitionsRDD[47] at sql at NativeMethodAccessorImpl.java:-2), which has no missing parents
17/03/28 12:32:16 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 26.8 KB, free 966.2 KB)
17/03/28 12:32:16 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 11.0 KB, free 977.2 KB)
17/03/28 12:32:16 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on localhost:51431 (size: 11.0 KB, free: 511.0 MB)
17/03/28 12:32:16 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1006
17/03/28 12:32:16 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[47] at sql at NativeMethodAccessorImpl.java:-2)
17/03/28 12:32:16 INFO TaskSchedulerImpl: Adding task set 11.0 with 2 tasks
17/03/28 12:32:16 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 13, localhost, partition 0,PROCESS_LOCAL, 2471 bytes)
17/03/28 12:32:16 INFO TaskSetManager: Starting task 1.0 in stage 11.0 (TID 14, localhost, partition 1,PROCESS_LOCAL, 2471 bytes)
17/03/28 12:32:16 INFO Executor: Running task 0.0 in stage 11.0 (TID 13)
17/03/28 12:32:16 INFO Executor: Running task 1.0 in stage 11.0 (TID 14)
17/03/28 12:32:16 INFO CacheManager: Partition rdd_44_0 not found, computing it
17/03/28 12:32:16 INFO HadoopRDD: Input split: file:/C:/Users/lifeng/AppData/Local/Temp/RtmpobpWe6/spark_serialize_bdf64b31db10334a157167e3720fb5fda677e92a85e2e167238fb89c721869ff.csv:0+16824941
17/03/28 12:32:16 INFO CacheManager: Partition rdd_44_1 not found, computing it
17/03/28 12:32:16 INFO HadoopRDD: Input split: file:/C:/Users/lifeng/AppData/Local/Temp/RtmpobpWe6/spark_serialize_bdf64b31db10334a157167e3720fb5fda677e92a85e2e167238fb89c721869ff.csv:16824941+16824942
17/03/28 12:32:16 INFO GenerateUnsafeProjection: Code generated in 26.4703 ms
17/03/28 12:32:16 INFO BlockManagerInfo: Removed broadcast_15_piece0 on localhost:51431 in memory (size: 1944.0 B, free: 511.0 MB)
17/03/28 12:32:16 INFO ContextCleaner: Cleaned accumulator 31
17/03/28 12:32:16 INFO BlockManagerInfo: Removed broadcast_14_piece0 on localhost:51431 in memory (size: 19.3 KB, free: 511.0 MB)
17/03/28 12:32:16 INFO BlockManagerInfo: Removed broadcast_13_piece0 on localhost:51431 in memory (size: 1945.0 B, free: 511.0 MB)
17/03/28 12:32:16 INFO ContextCleaner: Cleaned accumulator 30
17/03/28 12:32:16 INFO BlockManagerInfo: Removed broadcast_12_piece0 on localhost:51431 in memory (size: 19.3 KB, free: 511.1 MB)
17/03/28 12:32:16 INFO BlockManagerInfo: Removed broadcast_11_piece0 on localhost:51431 in memory (size: 3.0 KB, free: 511.1 MB)
17/03/28 12:32:16 INFO ContextCleaner: Cleaned accumulator 29
17/03/28 12:32:16 INFO ContextCleaner: Cleaned accumulator 28
17/03/28 12:32:16 INFO BlockManagerInfo: Removed broadcast_10_piece0 on localhost:51431 in memory (size: 1224.0 B, free: 511.1 MB)
17/03/28 12:32:16 INFO ContextCleaner: Cleaned accumulator 27
17/03/28 12:32:21 INFO MemoryStore: Block rdd_44_0 stored as values in memory (estimated size 12.2 MB, free 12.7 MB)
17/03/28 12:32:21 INFO BlockManagerInfo: Added rdd_44_0 in memory on localhost:51431 (size: 12.2 MB, free: 498.9 MB)
17/03/28 12:32:21 INFO MemoryStore: Block rdd_44_1 stored as values in memory (estimated size 12.2 MB, free 24.9 MB)
17/03/28 12:32:21 INFO BlockManagerInfo: Added rdd_44_1 in memory on localhost:51431 (size: 12.2 MB, free: 486.7 MB)
17/03/28 12:32:21 INFO Executor: Finished task 0.0 in stage 11.0 (TID 13). 21077 bytes result sent to driver
17/03/28 12:32:21 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 13) in 5253 ms on localhost (1/2)
17/03/28 12:32:21 INFO Executor: Finished task 1.0 in stage 11.0 (TID 14). 21132 bytes result sent to driver
17/03/28 12:32:21 INFO DAGScheduler: ShuffleMapStage 11 (sql at NativeMethodAccessorImpl.java:-2) finished in 5.272 s
17/03/28 12:32:21 INFO TaskSetManager: Finished task 1.0 in stage 11.0 (TID 14) in 5271 ms on localhost (2/2)
17/03/28 12:32:21 INFO DAGScheduler: looking for newly runnable stages
17/03/28 12:32:21 INFO DAGScheduler: running: Set()
17/03/28 12:32:21 INFO DAGScheduler: waiting: Set(ResultStage 12)
17/03/28 12:32:21 INFO DAGScheduler: failed: Set()
17/03/28 12:32:21 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[50] at sql at NativeMethodAccessorImpl.java:-2), which has no missing parents
17/03/28 12:32:21 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 
17/03/28 12:32:21 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 9.3 KB, free 24.9 MB)
17/03/28 12:32:21 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 4.6 KB, free 24.9 MB)
17/03/28 12:32:21 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on localhost:51431 (size: 4.6 KB, free: 486.7 MB)
17/03/28 12:32:21 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1006
17/03/28 12:32:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[50] at sql at NativeMethodAccessorImpl.java:-2)
17/03/28 12:32:21 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks
17/03/28 12:32:21 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 15, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/03/28 12:32:21 INFO Executor: Running task 0.0 in stage 12.0 (TID 15)
17/03/28 12:32:21 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/03/28 12:32:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/03/28 12:32:21 INFO Executor: Finished task 0.0 in stage 12.0 (TID 15). 1830 bytes result sent to driver
17/03/28 12:32:21 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 15) in 7 ms on localhost (1/1)
17/03/28 12:32:21 INFO DAGScheduler: ResultStage 12 (sql at NativeMethodAccessorImpl.java:-2) finished in 0.007 s
17/03/28 12:32:21 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool 
17/03/28 12:32:21 INFO DAGScheduler: Job 9 finished: sql at NativeMethodAccessorImpl.java:-2, took 5.291930 s
17/03/28 12:32:21 INFO ParseDriver: Parsing command: SELECT count(*) FROM  `flights`
17/03/28 12:32:21 INFO ParseDriver: Parse Completed
17/03/28 12:32:21 INFO SparkContext: Starting job: collect at utils.scala:195
17/03/28 12:32:21 INFO DAGScheduler: Registering RDD 54 (collect at utils.scala:195)
17/03/28 12:32:21 INFO DAGScheduler: Got job 10 (collect at utils.scala:195) with 1 output partitions
17/03/28 12:32:21 INFO DAGScheduler: Final stage: ResultStage 14 (collect at utils.scala:195)
17/03/28 12:32:21 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 13)
17/03/28 12:32:21 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 13)
17/03/28 12:32:21 INFO DAGScheduler: Submitting ShuffleMapStage 13 (MapPartitionsRDD[54] at collect at utils.scala:195), which has no missing parents
17/03/28 12:32:21 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 26.9 KB, free 24.9 MB)
17/03/28 12:32:21 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 11.0 KB, free 24.9 MB)
17/03/28 12:32:21 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on localhost:51431 (size: 11.0 KB, free: 486.7 MB)
17/03/28 12:32:21 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1006
17/03/28 12:32:21 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 13 (MapPartitionsRDD[54] at collect at utils.scala:195)
17/03/28 12:32:21 INFO TaskSchedulerImpl: Adding task set 13.0 with 2 tasks
17/03/28 12:32:21 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 16, localhost, partition 0,PROCESS_LOCAL, 2471 bytes)
17/03/28 12:32:21 INFO TaskSetManager: Starting task 1.0 in stage 13.0 (TID 17, localhost, partition 1,PROCESS_LOCAL, 2471 bytes)
17/03/28 12:32:21 INFO Executor: Running task 0.0 in stage 13.0 (TID 16)
17/03/28 12:32:21 INFO Executor: Running task 1.0 in stage 13.0 (TID 17)
17/03/28 12:32:21 INFO BlockManager: Found block rdd_44_0 locally
17/03/28 12:32:21 INFO BlockManager: Found block rdd_44_1 locally
17/03/28 12:32:21 INFO Executor: Finished task 1.0 in stage 13.0 (TID 17). 2679 bytes result sent to driver
17/03/28 12:32:21 INFO TaskSetManager: Finished task 1.0 in stage 13.0 (TID 17) in 28 ms on localhost (1/2)
17/03/28 12:32:21 INFO Executor: Finished task 0.0 in stage 13.0 (TID 16). 2679 bytes result sent to driver
17/03/28 12:32:21 INFO DAGScheduler: ShuffleMapStage 13 (collect at utils.scala:195) finished in 0.034 s
17/03/28 12:32:21 INFO DAGScheduler: looking for newly runnable stages
17/03/28 12:32:21 INFO DAGScheduler: running: Set()
17/03/28 12:32:21 INFO DAGScheduler: waiting: Set(ResultStage 14)
17/03/28 12:32:21 INFO DAGScheduler: failed: Set()
17/03/28 12:32:21 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 16) in 33 ms on localhost (2/2)
17/03/28 12:32:21 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool 
17/03/28 12:32:21 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[57] at collect at utils.scala:195), which has no missing parents
17/03/28 12:32:21 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 9.4 KB, free 24.9 MB)
17/03/28 12:32:21 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 4.6 KB, free 24.9 MB)
17/03/28 12:32:21 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on localhost:51431 (size: 4.6 KB, free: 486.7 MB)
17/03/28 12:32:21 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1006
17/03/28 12:32:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[57] at collect at utils.scala:195)
17/03/28 12:32:21 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks
17/03/28 12:32:21 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 18, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/03/28 12:32:21 INFO Executor: Running task 0.0 in stage 14.0 (TID 18)
17/03/28 12:32:21 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/03/28 12:32:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/03/28 12:32:21 INFO Executor: Finished task 0.0 in stage 14.0 (TID 18). 1830 bytes result sent to driver
17/03/28 12:32:21 INFO DAGScheduler: ResultStage 14 (collect at utils.scala:195) finished in 0.008 s
17/03/28 12:32:21 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 18) in 8 ms on localhost (1/1)
17/03/28 12:32:21 INFO DAGScheduler: Job 10 finished: collect at utils.scala:195, took 0.058279 s
17/03/28 12:32:21 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool 
17/03/28 12:32:21 INFO ParseDriver: Parsing command: SELECT *
FROM `flights` AS `zzz2`
WHERE (0 = 1)
17/03/28 12:32:21 INFO ParseDriver: Parse Completed
17/03/28 12:32:21 INFO ParseDriver: Parsing command: SELECT * FROM iris LIMIT 5
17/03/28 12:32:21 INFO ParseDriver: Parse Completed
17/03/28 12:32:21 INFO SparkContext: Starting job: collect at utils.scala:195
17/03/28 12:32:21 INFO DAGScheduler: Got job 11 (collect at utils.scala:195) with 1 output partitions
17/03/28 12:32:21 INFO DAGScheduler: Final stage: ResultStage 15 (collect at utils.scala:195)
17/03/28 12:32:21 INFO DAGScheduler: Parents of final stage: List()
17/03/28 12:32:21 INFO DAGScheduler: Missing parents: List()
17/03/28 12:32:21 INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[60] at collect at utils.scala:195), which has no missing parents
17/03/28 12:32:21 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 14.0 KB, free 24.9 MB)
17/03/28 12:32:21 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 6.8 KB, free 24.9 MB)
17/03/28 12:32:21 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on localhost:51431 (size: 6.8 KB, free: 486.7 MB)
17/03/28 12:32:21 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1006
17/03/28 12:32:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[60] at collect at utils.scala:195)
17/03/28 12:32:21 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks
17/03/28 12:32:21 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 19, localhost, partition 0,PROCESS_LOCAL, 2482 bytes)
17/03/28 12:32:21 INFO Executor: Running task 0.0 in stage 15.0 (TID 19)
17/03/28 12:32:21 INFO BlockManager: Found block rdd_14_0 locally
17/03/28 12:32:21 INFO GenerateColumnAccessor: Code generated in 13.517026 ms
17/03/28 12:32:21 INFO GenerateSafeProjection: Code generated in 6.688785 ms
17/03/28 12:32:21 INFO Executor: Finished task 0.0 in stage 15.0 (TID 19). 3011 bytes result sent to driver
17/03/28 12:32:21 INFO DAGScheduler: ResultStage 15 (collect at utils.scala:195) finished in 0.038 s
17/03/28 12:32:21 INFO DAGScheduler: Job 11 finished: collect at utils.scala:195, took 0.043120 s
17/03/28 12:32:21 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 19) in 37 ms on localhost (1/1)
17/03/28 12:32:21 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool 
17/03/28 12:32:21 INFO HiveMetaStore: 0: get_tables: db=default pat=.*
17/03/28 12:32:21 INFO audit: ugi=lifeng	ip=unknown-ip-addr	cmd=get_tables: db=default pat=.*	
17/03/28 12:32:22 INFO SparkContext: Starting job: collect at utils.scala:195
17/03/28 12:32:22 INFO DAGScheduler: Got job 12 (collect at utils.scala:195) with 1 output partitions
17/03/28 12:32:22 INFO DAGScheduler: Final stage: ResultStage 16 (collect at utils.scala:195)
17/03/28 12:32:22 INFO DAGScheduler: Parents of final stage: List()
17/03/28 12:32:22 INFO DAGScheduler: Missing parents: List()
17/03/28 12:32:22 INFO DAGScheduler: Submitting ResultStage 16 (MapPartitionsRDD[62] at collect at utils.scala:195), which has no missing parents
17/03/28 12:32:22 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 1968.0 B, free 24.9 MB)
17/03/28 12:32:22 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 1224.0 B, free 24.9 MB)
17/03/28 12:32:22 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on localhost:51431 (size: 1224.0 B, free: 486.7 MB)
17/03/28 12:32:22 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1006
17/03/28 12:32:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[62] at collect at utils.scala:195)
17/03/28 12:32:22 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks
17/03/28 12:32:22 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 20, localhost, partition 0,PROCESS_LOCAL, 2687 bytes)
17/03/28 12:32:22 INFO Executor: Running task 0.0 in stage 16.0 (TID 20)
17/03/28 12:32:22 INFO Executor: Finished task 0.0 in stage 16.0 (TID 20). 1299 bytes result sent to driver
17/03/28 12:32:22 INFO DAGScheduler: ResultStage 16 (collect at utils.scala:195) finished in 0.004 s
17/03/28 12:32:22 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 20) in 3 ms on localhost (1/1)
17/03/28 12:32:22 INFO DAGScheduler: Job 12 finished: collect at utils.scala:195, took 0.007805 s
17/03/28 12:32:22 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool 
17/03/28 12:32:30 INFO HiveMetaStore: 0: get_tables: db=default pat=.*
17/03/28 12:32:30 INFO audit: ugi=lifeng	ip=unknown-ip-addr	cmd=get_tables: db=default pat=.*	
17/03/28 12:32:30 INFO SparkContext: Starting job: collect at utils.scala:59
17/03/28 12:32:30 INFO DAGScheduler: Got job 13 (collect at utils.scala:59) with 1 output partitions
17/03/28 12:32:30 INFO DAGScheduler: Final stage: ResultStage 17 (collect at utils.scala:59)
17/03/28 12:32:30 INFO DAGScheduler: Parents of final stage: List()
17/03/28 12:32:30 INFO DAGScheduler: Missing parents: List()
17/03/28 12:32:30 INFO DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[66] at map at utils.scala:56), which has no missing parents
17/03/28 12:32:30 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 5.4 KB, free 25.0 MB)
17/03/28 12:32:30 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 3.0 KB, free 25.0 MB)
17/03/28 12:32:30 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on localhost:51431 (size: 3.0 KB, free: 486.7 MB)
17/03/28 12:32:30 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1006
17/03/28 12:32:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[66] at map at utils.scala:56)
17/03/28 12:32:30 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks
17/03/28 12:32:30 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 21, localhost, partition 0,PROCESS_LOCAL, 2687 bytes)
17/03/28 12:32:30 INFO Executor: Running task 0.0 in stage 17.0 (TID 21)
17/03/28 12:32:30 INFO Executor: Finished task 0.0 in stage 17.0 (TID 21). 1077 bytes result sent to driver
17/03/28 12:32:30 INFO DAGScheduler: ResultStage 17 (collect at utils.scala:59) finished in 0.004 s
17/03/28 12:32:30 INFO DAGScheduler: Job 13 finished: collect at utils.scala:59, took 0.010687 s
17/03/28 12:32:30 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 21) in 4 ms on localhost (1/1)
17/03/28 12:32:30 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool 
17/03/28 12:32:31 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 208.5 KB, free 25.2 MB)
17/03/28 12:32:31 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 19.3 KB, free 25.2 MB)
17/03/28 12:32:31 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on localhost:51431 (size: 19.3 KB, free: 486.7 MB)
17/03/28 12:32:31 INFO SparkContext: Created broadcast 24 from textFile at TextFile.scala:30
17/03/28 12:32:31 INFO FileInputFormat: Total input paths to process : 1
17/03/28 12:32:31 INFO SparkContext: Starting job: take at CsvRelation.scala:249
17/03/28 12:32:31 INFO DAGScheduler: Got job 14 (take at CsvRelation.scala:249) with 1 output partitions
17/03/28 12:32:31 INFO DAGScheduler: Final stage: ResultStage 18 (take at CsvRelation.scala:249)
17/03/28 12:32:31 INFO DAGScheduler: Parents of final stage: List()
17/03/28 12:32:31 INFO DAGScheduler: Missing parents: List()
17/03/28 12:32:31 INFO DAGScheduler: Submitting ResultStage 18 (C:\Users\lifeng\AppData\Local\Temp\RtmpobpWe6/spark_serialize_a7860fa36853b90b80e72911eab9ae08d70b911c204b0a95bf637bdc36920906.csv MapPartitionsRDD[68] at textFile at TextFile.scala:30), which has no missing parents
17/03/28 12:32:31 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 3.2 KB, free 25.2 MB)
17/03/28 12:32:31 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 1945.0 B, free 25.2 MB)
17/03/28 12:32:31 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on localhost:51431 (size: 1945.0 B, free: 486.6 MB)
17/03/28 12:32:31 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1006
17/03/28 12:32:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (C:\Users\lifeng\AppData\Local\Temp\RtmpobpWe6/spark_serialize_a7860fa36853b90b80e72911eab9ae08d70b911c204b0a95bf637bdc36920906.csv MapPartitionsRDD[68] at textFile at TextFile.scala:30)
17/03/28 12:32:31 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks
17/03/28 12:32:31 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 22, localhost, partition 0,PROCESS_LOCAL, 2482 bytes)
17/03/28 12:32:31 INFO Executor: Running task 0.0 in stage 18.0 (TID 22)
17/03/28 12:32:31 INFO HadoopRDD: Input split: file:/C:/Users/lifeng/AppData/Local/Temp/RtmpobpWe6/spark_serialize_a7860fa36853b90b80e72911eab9ae08d70b911c204b0a95bf637bdc36920906.csv:0+3427780
17/03/28 12:32:31 INFO Executor: Finished task 0.0 in stage 18.0 (TID 22). 2768 bytes result sent to driver
17/03/28 12:32:31 INFO DAGScheduler: ResultStage 18 (take at CsvRelation.scala:249) finished in 0.005 s
17/03/28 12:32:31 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 22) in 5 ms on localhost (1/1)
17/03/28 12:32:31 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool 
17/03/28 12:32:31 INFO DAGScheduler: Job 14 finished: take at CsvRelation.scala:249, took 0.021337 s
17/03/28 12:32:31 INFO ParseDriver: Parsing command: SELECT * FROM  `batting`
17/03/28 12:32:31 INFO ParseDriver: Parse Completed
17/03/28 12:32:31 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 208.5 KB, free 25.4 MB)
17/03/28 12:32:31 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 19.3 KB, free 25.4 MB)
17/03/28 12:32:31 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on localhost:51431 (size: 19.3 KB, free: 486.6 MB)
17/03/28 12:32:31 INFO SparkContext: Created broadcast 26 from textFile at TextFile.scala:30
17/03/28 12:32:31 INFO FileInputFormat: Total input paths to process : 1
17/03/28 12:32:31 INFO SparkContext: Starting job: take at CsvRelation.scala:249
17/03/28 12:32:31 INFO DAGScheduler: Got job 15 (take at CsvRelation.scala:249) with 1 output partitions
17/03/28 12:32:31 INFO DAGScheduler: Final stage: ResultStage 19 (take at CsvRelation.scala:249)
17/03/28 12:32:31 INFO DAGScheduler: Parents of final stage: List()
17/03/28 12:32:31 INFO DAGScheduler: Missing parents: List()
17/03/28 12:32:31 INFO DAGScheduler: Submitting ResultStage 19 (C:\Users\lifeng\AppData\Local\Temp\RtmpobpWe6/spark_serialize_a7860fa36853b90b80e72911eab9ae08d70b911c204b0a95bf637bdc36920906.csv MapPartitionsRDD[70] at textFile at TextFile.scala:30), which has no missing parents
17/03/28 12:32:31 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 3.2 KB, free 25.4 MB)
17/03/28 12:32:31 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 1945.0 B, free 25.4 MB)
17/03/28 12:32:31 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on localhost:51431 (size: 1945.0 B, free: 486.6 MB)
17/03/28 12:32:31 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1006
17/03/28 12:32:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (C:\Users\lifeng\AppData\Local\Temp\RtmpobpWe6/spark_serialize_a7860fa36853b90b80e72911eab9ae08d70b911c204b0a95bf637bdc36920906.csv MapPartitionsRDD[70] at textFile at TextFile.scala:30)
17/03/28 12:32:31 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks
17/03/28 12:32:31 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 23, localhost, partition 0,PROCESS_LOCAL, 2482 bytes)
17/03/28 12:32:31 INFO Executor: Running task 0.0 in stage 19.0 (TID 23)
17/03/28 12:32:31 INFO HadoopRDD: Input split: file:/C:/Users/lifeng/AppData/Local/Temp/RtmpobpWe6/spark_serialize_a7860fa36853b90b80e72911eab9ae08d70b911c204b0a95bf637bdc36920906.csv:0+3427780
17/03/28 12:32:31 INFO Executor: Finished task 0.0 in stage 19.0 (TID 23). 2768 bytes result sent to driver
17/03/28 12:32:31 INFO DAGScheduler: ResultStage 19 (take at CsvRelation.scala:249) finished in 0.015 s
17/03/28 12:32:31 INFO DAGScheduler: Job 15 finished: take at CsvRelation.scala:249, took 0.021318 s
17/03/28 12:32:31 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 23) in 15 ms on localhost (1/1)
17/03/28 12:32:31 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool 
17/03/28 12:32:31 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 208.5 KB, free 25.6 MB)
17/03/28 12:32:31 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 19.3 KB, free 25.6 MB)
17/03/28 12:32:31 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on localhost:51431 (size: 19.3 KB, free: 486.6 MB)
17/03/28 12:32:31 INFO SparkContext: Created broadcast 28 from textFile at TextFile.scala:30
17/03/28 12:32:31 INFO FileInputFormat: Total input paths to process : 1
17/03/28 12:32:31 INFO SparkContext: Starting job: sql at NativeMethodAccessorImpl.java:-2
17/03/28 12:32:31 INFO DAGScheduler: Registering RDD 80 (sql at NativeMethodAccessorImpl.java:-2)
17/03/28 12:32:31 INFO DAGScheduler: Got job 16 (sql at NativeMethodAccessorImpl.java:-2) with 1 output partitions
17/03/28 12:32:31 INFO DAGScheduler: Final stage: ResultStage 21 (sql at NativeMethodAccessorImpl.java:-2)
17/03/28 12:32:31 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 20)
17/03/28 12:32:31 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 20)
17/03/28 12:32:31 INFO DAGScheduler: Submitting ShuffleMapStage 20 (MapPartitionsRDD[80] at sql at NativeMethodAccessorImpl.java:-2), which has no missing parents
17/03/28 12:32:31 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 27.9 KB, free 25.7 MB)
17/03/28 12:32:31 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 11.2 KB, free 25.7 MB)
17/03/28 12:32:31 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on localhost:51431 (size: 11.2 KB, free: 486.6 MB)
17/03/28 12:32:31 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1006
17/03/28 12:32:31 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 20 (MapPartitionsRDD[80] at sql at NativeMethodAccessorImpl.java:-2)
17/03/28 12:32:31 INFO TaskSchedulerImpl: Adding task set 20.0 with 2 tasks
17/03/28 12:32:31 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 24, localhost, partition 0,PROCESS_LOCAL, 2471 bytes)
17/03/28 12:32:31 INFO TaskSetManager: Starting task 1.0 in stage 20.0 (TID 25, localhost, partition 1,PROCESS_LOCAL, 2471 bytes)
17/03/28 12:32:32 INFO Executor: Running task 0.0 in stage 20.0 (TID 24)
17/03/28 12:32:32 INFO Executor: Running task 1.0 in stage 20.0 (TID 25)
17/03/28 12:32:32 INFO CacheManager: Partition rdd_77_1 not found, computing it
17/03/28 12:32:32 INFO HadoopRDD: Input split: file:/C:/Users/lifeng/AppData/Local/Temp/RtmpobpWe6/spark_serialize_a7860fa36853b90b80e72911eab9ae08d70b911c204b0a95bf637bdc36920906.csv:3427780+3427781
17/03/28 12:32:32 INFO CacheManager: Partition rdd_77_0 not found, computing it
17/03/28 12:32:32 INFO HadoopRDD: Input split: file:/C:/Users/lifeng/AppData/Local/Temp/RtmpobpWe6/spark_serialize_a7860fa36853b90b80e72911eab9ae08d70b911c204b0a95bf637bdc36920906.csv:0+3427780
17/03/28 12:32:32 INFO GenerateUnsafeProjection: Code generated in 16.343296 ms
17/03/28 12:32:32 INFO ContextCleaner: Cleaned accumulator 54
17/03/28 12:32:32 INFO BlockManagerInfo: Removed broadcast_27_piece0 on localhost:51431 in memory (size: 1945.0 B, free: 486.6 MB)
17/03/28 12:32:32 INFO ContextCleaner: Cleaned accumulator 58
17/03/28 12:32:32 INFO BlockManagerInfo: Removed broadcast_26_piece0 on localhost:51431 in memory (size: 19.3 KB, free: 486.6 MB)
17/03/28 12:32:32 INFO BlockManagerInfo: Removed broadcast_25_piece0 on localhost:51431 in memory (size: 1945.0 B, free: 486.6 MB)
17/03/28 12:32:32 INFO ContextCleaner: Cleaned accumulator 57
17/03/28 12:32:32 INFO BlockManagerInfo: Removed broadcast_24_piece0 on localhost:51431 in memory (size: 19.3 KB, free: 486.6 MB)
17/03/28 12:32:32 INFO BlockManagerInfo: Removed broadcast_23_piece0 on localhost:51431 in memory (size: 3.0 KB, free: 486.6 MB)
17/03/28 12:32:32 INFO ContextCleaner: Cleaned accumulator 56
17/03/28 12:32:32 INFO ContextCleaner: Cleaned accumulator 55
17/03/28 12:32:32 INFO BlockManagerInfo: Removed broadcast_22_piece0 on localhost:51431 in memory (size: 1224.0 B, free: 486.6 MB)
17/03/28 12:32:32 INFO BlockManagerInfo: Removed broadcast_21_piece0 on localhost:51431 in memory (size: 6.8 KB, free: 486.7 MB)
17/03/28 12:32:32 INFO ContextCleaner: Cleaned accumulator 53
17/03/28 12:32:32 INFO BlockManagerInfo: Removed broadcast_20_piece0 on localhost:51431 in memory (size: 4.6 KB, free: 486.7 MB)
17/03/28 12:32:32 INFO ContextCleaner: Cleaned accumulator 52
17/03/28 12:32:32 INFO BlockManagerInfo: Removed broadcast_19_piece0 on localhost:51431 in memory (size: 11.0 KB, free: 486.7 MB)
17/03/28 12:32:32 INFO ContextCleaner: Cleaned accumulator 51
17/03/28 12:32:32 INFO ContextCleaner: Cleaned shuffle 3
17/03/28 12:32:32 INFO BlockManagerInfo: Removed broadcast_18_piece0 on localhost:51431 in memory (size: 4.6 KB, free: 486.7 MB)
17/03/28 12:32:32 INFO MemoryStore: Block rdd_77_0 stored as values in memory (estimated size 1897.3 KB, free 27.0 MB)
17/03/28 12:32:32 INFO BlockManagerInfo: Added rdd_77_0 in memory on localhost:51431 (size: 1897.3 KB, free: 484.8 MB)
17/03/28 12:32:32 INFO MemoryStore: Block rdd_77_1 stored as values in memory (estimated size 1699.8 KB, free 28.6 MB)
17/03/28 12:32:32 INFO BlockManagerInfo: Added rdd_77_1 in memory on localhost:51431 (size: 1699.8 KB, free: 483.2 MB)
17/03/28 12:32:32 INFO Executor: Finished task 0.0 in stage 20.0 (TID 24). 9713 bytes result sent to driver
17/03/28 12:32:32 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 24) in 838 ms on localhost (1/2)
17/03/28 12:32:32 INFO Executor: Finished task 1.0 in stage 20.0 (TID 25). 9862 bytes result sent to driver
17/03/28 12:32:32 INFO DAGScheduler: ShuffleMapStage 20 (sql at NativeMethodAccessorImpl.java:-2) finished in 0.846 s
17/03/28 12:32:32 INFO TaskSetManager: Finished task 1.0 in stage 20.0 (TID 25) in 844 ms on localhost (2/2)
17/03/28 12:32:32 INFO DAGScheduler: looking for newly runnable stages
17/03/28 12:32:32 INFO DAGScheduler: running: Set()
17/03/28 12:32:32 INFO DAGScheduler: waiting: Set(ResultStage 21)
17/03/28 12:32:32 INFO DAGScheduler: failed: Set()
17/03/28 12:32:32 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool 
17/03/28 12:32:32 INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[83] at sql at NativeMethodAccessorImpl.java:-2), which has no missing parents
17/03/28 12:32:32 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 9.3 KB, free 28.6 MB)
17/03/28 12:32:32 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 4.6 KB, free 28.6 MB)
17/03/28 12:32:32 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on localhost:51431 (size: 4.6 KB, free: 483.2 MB)
17/03/28 12:32:32 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1006
17/03/28 12:32:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[83] at sql at NativeMethodAccessorImpl.java:-2)
17/03/28 12:32:32 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks
17/03/28 12:32:32 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 26, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/03/28 12:32:32 INFO Executor: Running task 0.0 in stage 21.0 (TID 26)
17/03/28 12:32:32 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/03/28 12:32:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/03/28 12:32:32 INFO Executor: Finished task 0.0 in stage 21.0 (TID 26). 1830 bytes result sent to driver
17/03/28 12:32:32 INFO DAGScheduler: ResultStage 21 (sql at NativeMethodAccessorImpl.java:-2) finished in 0.008 s
17/03/28 12:32:32 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 26) in 8 ms on localhost (1/1)
17/03/28 12:32:32 INFO DAGScheduler: Job 16 finished: sql at NativeMethodAccessorImpl.java:-2, took 0.864618 s
17/03/28 12:32:32 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool 
17/03/28 12:32:32 INFO ParseDriver: Parsing command: SELECT count(*) FROM  `batting`
17/03/28 12:32:32 INFO ParseDriver: Parse Completed
17/03/28 12:32:32 INFO SparkContext: Starting job: collect at utils.scala:195
17/03/28 12:32:32 INFO DAGScheduler: Registering RDD 87 (collect at utils.scala:195)
17/03/28 12:32:32 INFO DAGScheduler: Got job 17 (collect at utils.scala:195) with 1 output partitions
17/03/28 12:32:32 INFO DAGScheduler: Final stage: ResultStage 23 (collect at utils.scala:195)
17/03/28 12:32:32 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 22)
17/03/28 12:32:32 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 22)
17/03/28 12:32:32 INFO DAGScheduler: Submitting ShuffleMapStage 22 (MapPartitionsRDD[87] at collect at utils.scala:195), which has no missing parents
17/03/28 12:32:32 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 28.0 KB, free 28.7 MB)
17/03/28 12:32:32 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 11.2 KB, free 28.7 MB)
17/03/28 12:32:32 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on localhost:51431 (size: 11.2 KB, free: 483.1 MB)
17/03/28 12:32:32 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1006
17/03/28 12:32:32 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 22 (MapPartitionsRDD[87] at collect at utils.scala:195)
17/03/28 12:32:32 INFO TaskSchedulerImpl: Adding task set 22.0 with 2 tasks
17/03/28 12:32:32 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 27, localhost, partition 0,PROCESS_LOCAL, 2471 bytes)
17/03/28 12:32:32 INFO TaskSetManager: Starting task 1.0 in stage 22.0 (TID 28, localhost, partition 1,PROCESS_LOCAL, 2471 bytes)
17/03/28 12:32:32 INFO Executor: Running task 1.0 in stage 22.0 (TID 28)
17/03/28 12:32:32 INFO Executor: Running task 0.0 in stage 22.0 (TID 27)
17/03/28 12:32:32 INFO BlockManager: Found block rdd_77_1 locally
17/03/28 12:32:32 INFO BlockManager: Found block rdd_77_0 locally
17/03/28 12:32:32 INFO Executor: Finished task 0.0 in stage 22.0 (TID 27). 2679 bytes result sent to driver
17/03/28 12:32:32 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 27) in 30 ms on localhost (1/2)
17/03/28 12:32:32 INFO Executor: Finished task 1.0 in stage 22.0 (TID 28). 2679 bytes result sent to driver
17/03/28 12:32:32 INFO TaskSetManager: Finished task 1.0 in stage 22.0 (TID 28) in 34 ms on localhost (2/2)
17/03/28 12:32:32 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool 
17/03/28 12:32:32 INFO DAGScheduler: ShuffleMapStage 22 (collect at utils.scala:195) finished in 0.035 s
17/03/28 12:32:32 INFO DAGScheduler: looking for newly runnable stages
17/03/28 12:32:32 INFO DAGScheduler: running: Set()
17/03/28 12:32:32 INFO DAGScheduler: waiting: Set(ResultStage 23)
17/03/28 12:32:32 INFO DAGScheduler: failed: Set()
17/03/28 12:32:32 INFO DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[90] at collect at utils.scala:195), which has no missing parents
17/03/28 12:32:32 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 9.4 KB, free 28.7 MB)
17/03/28 12:32:32 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 4.7 KB, free 28.7 MB)
17/03/28 12:32:32 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on localhost:51431 (size: 4.7 KB, free: 483.1 MB)
17/03/28 12:32:32 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1006
17/03/28 12:32:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[90] at collect at utils.scala:195)
17/03/28 12:32:32 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks
17/03/28 12:32:32 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 29, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/03/28 12:32:32 INFO Executor: Running task 0.0 in stage 23.0 (TID 29)
17/03/28 12:32:32 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/03/28 12:32:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/03/28 12:32:32 INFO Executor: Finished task 0.0 in stage 23.0 (TID 29). 1830 bytes result sent to driver
17/03/28 12:32:32 INFO DAGScheduler: ResultStage 23 (collect at utils.scala:195) finished in 0.007 s
17/03/28 12:32:32 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 29) in 7 ms on localhost (1/1)
17/03/28 12:32:32 INFO DAGScheduler: Job 17 finished: collect at utils.scala:195, took 0.051737 s
17/03/28 12:32:32 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool 
17/03/28 12:32:32 INFO ParseDriver: Parsing command: SELECT *
FROM `batting` AS `zzz3`
WHERE (0 = 1)
17/03/28 12:32:32 INFO ParseDriver: Parse Completed
17/03/28 12:32:33 INFO HiveMetaStore: 0: get_tables: db=default pat=.*
17/03/28 12:32:33 INFO audit: ugi=lifeng	ip=unknown-ip-addr	cmd=get_tables: db=default pat=.*	
17/03/28 12:32:33 INFO SparkContext: Starting job: collect at utils.scala:59
17/03/28 12:32:33 INFO DAGScheduler: Got job 18 (collect at utils.scala:59) with 1 output partitions
17/03/28 12:32:33 INFO DAGScheduler: Final stage: ResultStage 24 (collect at utils.scala:59)
17/03/28 12:32:33 INFO DAGScheduler: Parents of final stage: List()
17/03/28 12:32:33 INFO DAGScheduler: Missing parents: List()
17/03/28 12:32:33 INFO DAGScheduler: Submitting ResultStage 24 (MapPartitionsRDD[94] at map at utils.scala:56), which has no missing parents
17/03/28 12:32:33 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 5.4 KB, free 28.7 MB)
17/03/28 12:32:33 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 3.0 KB, free 28.7 MB)
17/03/28 12:32:33 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on localhost:51431 (size: 3.0 KB, free: 483.1 MB)
17/03/28 12:32:33 INFO SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1006
17/03/28 12:32:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[94] at map at utils.scala:56)
17/03/28 12:32:33 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks
17/03/28 12:32:33 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 30, localhost, partition 0,PROCESS_LOCAL, 2728 bytes)
17/03/28 12:32:33 INFO Executor: Running task 0.0 in stage 24.0 (TID 30)
17/03/28 12:32:33 INFO Executor: Finished task 0.0 in stage 24.0 (TID 30). 1087 bytes result sent to driver
17/03/28 12:32:33 INFO DAGScheduler: ResultStage 24 (collect at utils.scala:59) finished in 0.004 s
17/03/28 12:32:33 INFO DAGScheduler: Job 18 finished: collect at utils.scala:59, took 0.007449 s
17/03/28 12:32:33 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 30) in 4 ms on localhost (1/1)
17/03/28 12:32:33 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool 
17/03/28 12:32:33 INFO HiveMetaStore: 0: get_tables: db=default pat=.*
17/03/28 12:32:33 INFO audit: ugi=lifeng	ip=unknown-ip-addr	cmd=get_tables: db=default pat=.*	
17/03/28 12:32:33 INFO SparkContext: Starting job: collect at utils.scala:195
17/03/28 12:32:33 INFO DAGScheduler: Got job 19 (collect at utils.scala:195) with 1 output partitions
17/03/28 12:32:33 INFO DAGScheduler: Final stage: ResultStage 25 (collect at utils.scala:195)
17/03/28 12:32:33 INFO DAGScheduler: Parents of final stage: List()
17/03/28 12:32:33 INFO DAGScheduler: Missing parents: List()
17/03/28 12:32:33 INFO DAGScheduler: Submitting ResultStage 25 (MapPartitionsRDD[96] at collect at utils.scala:195), which has no missing parents
17/03/28 12:32:33 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 1968.0 B, free 28.7 MB)
17/03/28 12:32:33 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 1224.0 B, free 28.7 MB)
17/03/28 12:32:33 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on localhost:51431 (size: 1224.0 B, free: 483.1 MB)
17/03/28 12:32:33 INFO SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1006
17/03/28 12:32:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[96] at collect at utils.scala:195)
17/03/28 12:32:33 INFO TaskSchedulerImpl: Adding task set 25.0 with 1 tasks
17/03/28 12:32:33 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 31, localhost, partition 0,PROCESS_LOCAL, 2728 bytes)
17/03/28 12:32:33 INFO Executor: Running task 0.0 in stage 25.0 (TID 31)
17/03/28 12:32:33 INFO Executor: Finished task 0.0 in stage 25.0 (TID 31). 1340 bytes result sent to driver
17/03/28 12:32:33 INFO DAGScheduler: ResultStage 25 (collect at utils.scala:195) finished in 0.002 s
17/03/28 12:32:33 INFO DAGScheduler: Job 19 finished: collect at utils.scala:195, took 0.007211 s
17/03/28 12:32:33 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 31) in 2 ms on localhost (1/1)
17/03/28 12:32:33 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool 
17/03/28 12:33:03 INFO BlockManagerInfo: Removed broadcast_32_piece0 on localhost:51431 in memory (size: 4.7 KB, free: 483.1 MB)
17/03/28 12:33:03 INFO BlockManagerInfo: Removed broadcast_30_piece0 on localhost:51431 in memory (size: 4.6 KB, free: 483.1 MB)
17/03/28 12:33:03 INFO ContextCleaner: Cleaned accumulator 79
17/03/28 12:33:03 INFO BlockManagerInfo: Removed broadcast_31_piece0 on localhost:51431 in memory (size: 11.2 KB, free: 483.2 MB)
17/03/28 12:33:03 INFO ContextCleaner: Cleaned accumulator 78
17/03/28 12:33:03 INFO ContextCleaner: Cleaned shuffle 5
17/03/28 12:33:03 INFO BlockManagerInfo: Removed broadcast_34_piece0 on localhost:51431 in memory (size: 1224.0 B, free: 483.2 MB)
17/03/28 12:33:03 INFO ContextCleaner: Cleaned accumulator 82
17/03/28 12:33:03 INFO BlockManagerInfo: Removed broadcast_33_piece0 on localhost:51431 in memory (size: 3.0 KB, free: 483.2 MB)
17/03/28 12:33:03 INFO ContextCleaner: Cleaned accumulator 81
17/03/28 12:33:03 INFO ContextCleaner: Cleaned accumulator 80
17/03/28 12:34:22 INFO SparkContext: Invoking stop() from shutdown hook
17/03/28 12:34:22 INFO SparkUI: Stopped Spark web UI at http://127.0.0.1:4040
17/03/28 12:34:22 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
17/03/28 12:34:22 INFO MemoryStore: MemoryStore cleared
17/03/28 12:34:22 INFO BlockManager: BlockManager stopped
17/03/28 12:34:22 INFO BlockManagerMaster: BlockManagerMaster stopped
17/03/28 12:34:22 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
17/03/28 12:34:22 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
17/03/28 12:34:22 WARN SparkEnv: Exception while deleting Spark temp dir: C:\Users\lifeng\AppData\Local\Temp\spark-30a2418b-c83a-4d4d-86af-0c5d2397bf7f\userFiles-b7d8cea0-c626-47d9-85b7-e90de11d84b3
java.io.IOException: Failed to delete: C:\Users\lifeng\AppData\Local\Temp\spark-30a2418b-c83a-4d4d-86af-0c5d2397bf7f\userFiles-b7d8cea0-c626-47d9-85b7-e90de11d84b3
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:929)
	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:119)
	at org.apache.spark.SparkContext$$anonfun$stop$12.apply$mcV$sp(SparkContext.scala:1756)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1219)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1755)
	at org.apache.spark.SparkContext$$anonfun$3.apply$mcV$sp(SparkContext.scala:596)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:267)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1801)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:218)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
17/03/28 12:34:22 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
17/03/28 12:34:22 INFO SparkContext: Successfully stopped SparkContext
17/03/28 12:34:22 INFO ShutdownHookManager: Shutdown hook called
17/03/28 12:34:22 INFO ShutdownHookManager: Deleting directory C:\Users\lifeng\AppData\Local\Temp\spark-30a2418b-c83a-4d4d-86af-0c5d2397bf7f\httpd-d9498df8-9833-4a70-85ea-d7d34a1fdd0d
17/03/28 12:34:22 INFO ShutdownHookManager: Deleting directory C:\Users\lifeng\AppData\Local\Temp\spark-80b3b8ba-1585-4075-a364-b76eb9d8ee56
17/03/28 12:34:22 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.
17/03/28 12:34:22 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\lifeng\AppData\Local\Temp\spark-80b3b8ba-1585-4075-a364-b76eb9d8ee56
java.io.IOException: Failed to delete: C:\Users\lifeng\AppData\Local\Temp\spark-80b3b8ba-1585-4075-a364-b76eb9d8ee56
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:929)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:267)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1801)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:218)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
17/03/28 12:34:22 INFO ShutdownHookManager: Deleting directory C:\Users\lifeng\AppData\Local\Temp\spark-30a2418b-c83a-4d4d-86af-0c5d2397bf7f\userFiles-b7d8cea0-c626-47d9-85b7-e90de11d84b3
17/03/28 12:34:22 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\lifeng\AppData\Local\Temp\spark-30a2418b-c83a-4d4d-86af-0c5d2397bf7f\userFiles-b7d8cea0-c626-47d9-85b7-e90de11d84b3
java.io.IOException: Failed to delete: C:\Users\lifeng\AppData\Local\Temp\spark-30a2418b-c83a-4d4d-86af-0c5d2397bf7f\userFiles-b7d8cea0-c626-47d9-85b7-e90de11d84b3
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:929)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:267)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1801)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:218)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
17/03/28 12:34:22 INFO ShutdownHookManager: Deleting directory C:\Users\lifeng\AppData\Local\Temp\spark-30a2418b-c83a-4d4d-86af-0c5d2397bf7f
17/03/28 12:34:22 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\lifeng\AppData\Local\Temp\spark-30a2418b-c83a-4d4d-86af-0c5d2397bf7f
java.io.IOException: Failed to delete: C:\Users\lifeng\AppData\Local\Temp\spark-30a2418b-c83a-4d4d-86af-0c5d2397bf7f
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:929)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:267)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1801)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:218)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
17/03/28 15:39:47 INFO SparkContext: Running Spark version 2.1.0
17/03/28 15:39:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/03/28 15:39:48 INFO SecurityManager: Changing view acls to: lifeng
17/03/28 15:39:48 INFO SecurityManager: Changing modify acls to: lifeng
17/03/28 15:39:48 INFO SecurityManager: Changing view acls groups to: 
17/03/28 15:39:48 INFO SecurityManager: Changing modify acls groups to: 
17/03/28 15:39:48 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lifeng); groups with view permissions: Set(); users  with modify permissions: Set(lifeng); groups with modify permissions: Set()
17/03/28 15:39:48 INFO Utils: Successfully started service 'sparkDriver' on port 50255.
17/03/28 15:39:48 INFO SparkEnv: Registering MapOutputTracker
17/03/28 15:39:48 INFO SparkEnv: Registering BlockManagerMaster
17/03/28 15:39:48 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/03/28 15:39:48 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/03/28 15:39:48 INFO DiskBlockManager: Created local directory at C:\Users\lifeng\AppData\Local\Temp\blockmgr-829e17ea-c00f-4eec-9533-b438c615e63d
17/03/28 15:39:48 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
17/03/28 15:39:48 INFO SparkEnv: Registering OutputCommitCoordinator
17/03/28 15:39:49 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/03/28 15:39:49 INFO SparkUI: Bound SparkUI to 127.0.0.1, and started at http://127.0.0.1:4040
17/03/28 15:39:49 INFO SparkContext: Added JAR file:/D:/R/R-3.3.3/library/sparklyr/java/sparklyr-2.1-2.11.jar at spark://127.0.0.1:50255/jars/sparklyr-2.1-2.11.jar with timestamp 1490686789297
17/03/28 15:39:49 INFO Executor: Starting executor ID driver on host localhost
17/03/28 15:39:49 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50276.
17/03/28 15:39:49 INFO NettyBlockTransferService: Server created on 127.0.0.1:50276
17/03/28 15:39:49 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/03/28 15:39:49 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 127.0.0.1, 50276, None)
17/03/28 15:39:49 INFO BlockManagerMasterEndpoint: Registering block manager 127.0.0.1:50276 with 366.3 MB RAM, BlockManagerId(driver, 127.0.0.1, 50276, None)
17/03/28 15:39:49 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 127.0.0.1, 50276, None)
17/03/28 15:39:49 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 127.0.0.1, 50276, None)
